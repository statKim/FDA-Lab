---
title: "Bootstrap aggregated sparse FPCA for classification"
author: "Hyunsung Kim"
date: Febrary 14, 2020
institute: Department of Statistics \newline Chung-Ang University
fonttheme: "professionalfonts"
output:
  beamer_presentation:
    theme: "metropolis"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(fdapace)
library(reshape2)
library(fda)
```

## How to perform sparse FPCA in \texttt{fdapace} package?
1. Estimate covariance function by **kernel smoothing**.
2. Conduct eigenanalysis.
3. Obtain FPC scores by **PACE** (Principal component Analysis through Conditional Expectation) method.


# Simulation studies

## Simulation studies

- [*Probability-enhanced effective dimension reduction for classifying sparse functional data* (Yao *et al.*)](https://link.springer.com/content/pdf/10.1007%2Fs11749-015-0470-2.pdf)
- 2 simulation models
  - Model \MakeUppercase{\romannumeral 2}
  - Model \MakeUppercase{\romannumeral 4}
- 700 curves are generated with 200 training and 500 test set.
- Bagged classifers are obtained from 100 bootstrap resamples.
- 100 Monte Carlo repetitions for each model


## Simulated data from model \MakeUppercase{\romannumeral 2}

```{r fig.cap="The simulated data obtained from Model 2", fig.dim=c(6,3)}
  set.seed(1)
  n <- 700   # number of observations
  j <- 1:50  # number of basis
  
  # parameters when generate class label
  b <- matrix(ifelse(j <= 2, 1, (j-2)^(-3)), ncol=1)
  
  ## generate curves
  data <- list()
  for (i in 1:n) {
    # random sparsify
    num.obs <- sample(10:20, 1)
    t <- sort(runif(num.obs, 0, 10))
    
    # 201~700 are test set
    if (i > 200) {
      range.train <- range(unlist(data$Lt[1:200]))
      while( (max(t) > range.train[2]) | (min(t) < range.train[1]) ) {
        t <- sort(runif(num.obs, 0, 10))
      }
    }
    
    # eigenfunctions
    phi <- sapply(j, function(x){
      if (x %% 2 == 0) {
        sqrt(1/5)*sin(pi*t*x/5)
      } else {
        sqrt(1/5)*cos(pi*t*x/5)
      }
    })
    
    # generate PC scores
    xi <- sapply(j, function(x){ rnorm(1, 0, sqrt( x^(-1.5) )) })
    
    # parameters when generate class label
    beta.1 <- phi %*% b
    beta.2 <- matrix(sqrt(3/10)*(t/5-1), ncol=1)
    
    # measurement error
    eps <- rnorm(num.obs, 0, sqrt(0.1))
    
    # generate the curve
    X <- xi %*% t(phi) + eps
    
    # generate class label
    eps <- rnorm(1, 0, sqrt(0.1))   # model error
    # fx <- sin(pi*(X %*% beta.1)/4)  # model 1
    fx <- exp((X %*% beta.1)/2)-1   # model 2
    # fx <- sin(pi*(X %*% beta.1)/3) + exp((X %*% beta.2)/3) - 1   # model 3
    # fx <- atan(pi*(X %*% beta.1)) + exp((X %*% beta.2)/3) - 1    # model 4
    y <- factor(ifelse(fx + eps < 0, 0, 1), levels=c(0, 1))
    
    data$id[[i]] <- rep(i, num.obs)
    data$y[[i]] <- rep(y, num.obs)
    data$Lt[[i]] <- t
    data$Ly[[i]] <- X
  }
  
  # plot the generated curves
  sapply(data, unlist) %>%
    data.frame() %>%
    mutate(y = ifelse(unlist(data$y) == 0, "G1", "G2")) %>%
    ggplot(aes(x=Lt, y=Ly, group=id, color=y)) +
    geom_line() +
    xlab("Time") +
    ylab("") +
    theme_bw() +
    theme(legend.title = element_blank())
```

## Results of model \MakeUppercase{\romannumeral 2}

\begin{table}[ht]
  \caption{The average classification error with standard error in percentage from 100 Monte Carlo repetitions for sparse data (Model \MakeUppercase{\romannumeral 2})}
  \centering
  \tiny
  \begin{tabular}{ccccccc}
    \hline
           & Logistic   & SVM      & SVM        &     &     & Naive \\
    Method & Regression & (Linear) & (Gaussian) & LDA & QDA & Bayes \\ 
    \hline
    Single        & 16.7 (2.33) & 16.8 (2.20) & 17.5 (2.76) & 16.6 (2.30) & 17.8 (2.56) & 18.4 (2.66) \\ 
    Majority vote & \textcolor{red}{15.6 (1.95)} & 15.9 (1.87) & 16.2 (2.28) & 15.8 (1.96) & 16.5 (2.14) & 17.3 (2.42) \\ 
    OOB weight    & 16.0 (2.02) & 16.2 (1.94) & 16.6 (2.28) & 16.1 (1.98) & 16.9 (2.09) & 17.7 (2.43) \\ 
    \hline
  \end{tabular}
\end{table}



## Simulated data from model \MakeUppercase{\romannumeral 4}

```{r fig.cap="The simulated data obtained from Model 4", fig.dim=c(6,3)}
  set.seed(1)
  n <- 700   # number of observations
  j <- 1:50  # number of basis
  
  # parameters when generate class label
  b <- matrix(ifelse(j <= 2, 1, (j-2)^(-3)), ncol=1)
  
  ## generate curves
  data <- list()
  for (i in 1:n) {
    # random sparsify
    num.obs <- sample(10:20, 1)
    t <- sort(runif(num.obs, 0, 10))
    
    # 201~700 are test set
    if (i > 200) {
      range.train <- range(unlist(data$Lt[1:200]))
      while( (max(t) > range.train[2]) | (min(t) < range.train[1]) ) {
        t <- sort(runif(num.obs, 0, 10))
      }
    }
    
    # eigenfunctions
    phi <- sapply(j, function(x){
      if (x %% 2 == 0) {
        sqrt(1/5)*sin(pi*t*x/5)
      } else {
        sqrt(1/5)*cos(pi*t*x/5)
      }
    })
    
    # generate PC scores
    xi <- sapply(j, function(x){ rnorm(1, 0, sqrt( x^(-1.5) )) })
    
    # parameters when generate class label
    beta.1 <- phi %*% b
    beta.2 <- matrix(sqrt(3/10)*(t/5-1), ncol=1)
    
    # measurement error
    eps <- rnorm(num.obs, 0, sqrt(0.1))
    
    # generate the curve
    X <- xi %*% t(phi) + eps
    
    # generate class label
    eps <- rnorm(1, 0, sqrt(0.1))   # model error
    # fx <- sin(pi*(X %*% beta.1)/4)  # model 1
    # fx <- exp((X %*% beta.1)/2)-1   # model 2
    # fx <- sin(pi*(X %*% beta.1)/3) + exp((X %*% beta.2)/3) - 1   # model 3
    fx <- atan(pi*(X %*% beta.1)) + exp((X %*% beta.2)/3) - 1    # model 4
    y <- factor(ifelse(fx + eps < 0, 0, 1), levels=c(0, 1))
    
    data$id[[i]] <- rep(i, num.obs)
    data$y[[i]] <- rep(y, num.obs)
    data$Lt[[i]] <- t
    data$Ly[[i]] <- X
  }
  
  # plot the generated curves
  sapply(data, unlist) %>%
    data.frame() %>%
    mutate(y = ifelse(unlist(data$y) == 0, "G1", "G2")) %>%
    ggplot(aes(x=Lt, y=Ly, group=id, color=y)) +
    geom_line() +
    xlab("Time") +
    ylab("") +
    theme_bw() +
    theme(legend.title = element_blank())
```


## Results of model \MakeUppercase{\romannumeral 4}

\begin{table}[ht]
  \caption{The average classification error with standard error in percentage from 100 Monte Carlo repetitions for sparse data (Model \MakeUppercase{\romannumeral 4})}
  \centering
  \tiny
  \begin{tabular}{ccccccc}
    \hline
           & Logistic   & SVM      & SVM        &     &     & Naive \\
    Method & Regression & (Linear) & (Gaussian) & LDA & QDA & Bayes \\ 
    \hline
    Single        & 12.8 (2.41) & 12.8 (2.40) & 13.3 (2.65) & 12.8 (2.40) & 13.8 (2.56) & 14.8 (2.74) \\ 
    Majority vote & 11.2 (1.84) & \textcolor{red}{11.1 (1.89)} & 11.5 (1.98) & 11.2 (1.85) & 11.9 (2.03) & 13.3 (2.36) \\ 
    OOB weight    & 11.6 (1.86) & 11.5 (1.90) & 12.0 (1.96) & 11.6 (1.86) & 12.3 (2.06) & 13.6 (2.35) \\ 
    \hline
  \end{tabular}
\end{table}


# Real data analysis

## Real data analysis

- 2 real data applications
  - **Berkely growth data**
    - 93 curves with 54 girls and 39 boys
    - Split to 62 training and 31 test set randomly.
    - Dense data
    - Randomly sparsify with 12~15
  - **Spinal bone mineral density data**
    - 280 curves with 470 females and 390 males
    - Split to 187 training and 93 test set randomly.
    - Sparse data
- Gender classification
- Bagged classifers are obtained from 100 bootstrap resamples.
- 100 Monte Carlo repetitions for each data


## Berkely growth data

```{r fig.cap="Berkely growth data sparsified randomly", fig.dim=c(6,3)}
### load data and preprocessing
data <- rbind(t(growth$hgtm),
              t(growth$hgtf))
y <- factor(c(rep(0, ncol(growth$hgtm)), 
              rep(1, ncol(growth$hgtf))), levels = c(0, 1))
time <- growth$age
N <- length(y)

# random sparsify
data <- Sparsify(data, time, sparsity = c(12, 15))
id <- unlist( mapply(function(x, y){ rep(x, y) },
                     1:N,
                     sapply(data$Lt, length)) )
time <- unlist(data$Lt)
val <- unlist(data$Ly)
y <- unlist( mapply(function(x, z){ rep(x, z) },
                    y,
                    sapply(data$Lt, length)) )
data <- data.frame(id, time, val, y)

# graph of data
data %>% 
  mutate(y = ifelse(y == 1, "Girl", "Boy")) %>% 
  ggplot(aes(x=time, y=val, group=id, color=y)) +
  geom_line() +
  xlab("Time") +
  ylab("Height") +
  theme_bw() +
  theme(legend.title = element_blank())
```


## Results of berkerly growth data

\begin{table}[ht]
  \caption{The average classification error with standard error in percentage from 100 Monte Carlo repetitions for berkerly growth data}
  \centering
  \tiny
  \begin{tabular}{ccccccc}
    \hline
           & Logistic   & SVM      & SVM        &     &     & Naive \\
    Method & Regression & (Linear) & (Gaussian) & LDA & QDA & Bayes \\ 
    \hline
    Single        & 7.3 (4.80) & 5.3 (3.20) & 5.7 (4.03) & 5.8 (3.34) & 5.6 (3.35) & 5.6 (3.90) \\ 
    Majority vote & 5.9 (4.12) & 4.9 (3.19) & 5.3 (3.51) & 5.4 (3.24) & \textcolor{red}{4.9 (3.57)} & 5.5 (3.96) \\ 
    OOB weight    & 5.9 (4.12) & 5.0 (3.22) & 5.4 (3.62) & 5.4 (3.27) & 4.9 (3.54) & 5.5 (3.96) \\ 
    \hline
  \end{tabular}
\end{table}


## Spinal bone mineral density data

```{r fig.cap="Spinal bone mineral density data", fig.dim=c(6,3)}
setwd("C:\\Users\\user\\Desktop\\KHS\\FDA-Lab\\Sparse FPCA\\Application\\bootstrap_aggregating")
### load data and preprocessing
data <- read.csv("../spnbmd.csv", header=T)

ind <- data %>%
  # filter(ethnic == "Hispanic") %>%
  group_by(idnum) %>%
  summarise(n=n()) %>%
  filter(n >= 2) %>%
  dplyr::select(idnum)
data <- data[which(data$idnum %in% ind$idnum), ]
data$sex <- ifelse(data$sex == "fem", "Female", "Male")

ggplot(data, aes(x=age, y=spnbmd, group=idnum, color=sex)) +
  geom_line() +
  theme_bw() +
  ylab("Spinal bone density") +
  xlab("Age") +
  theme(legend.title = element_blank())
```


## Results of spinal bone mineral density data

\begin{table}[ht]
  \caption{The average classification error with standard error in percentage from 100 Monte Carlo repetitions for spinal bone mineral density data}
  \centering
  \tiny
  \begin{tabular}{ccccccc}
    \hline
           & Logistic   & SVM      & SVM        &     &     & Naive \\
    Method & Regression & (Linear) & (Gaussian) & LDA & QDA & Bayes \\ 
    \hline
    Single        & 31.3 (4.30) & 32.0 (4.27) & 33.2 (4.71) & 31.4 (4.44) & 33.3 (4.10) & 32.3 (4.33) \\ 
    Majority vote & \textcolor{red}{30.2 (3.72)} & 30.8 (4.18) & 31.2 (3.88) & 30.4 (3.77) & 31.6 (3.78) & 30.9 (3.83) \\ 
    OOB weight    & 30.3 (3.71) & 30.8 (4.07) & 31.4 (3.81) & 30.5 (3.82) & 31.8 (3.71) & 30.9 (3.86) \\ 
    \hline
  \end{tabular}
\end{table}

