---
title: "Partially observed functional data"
author: "Hyunsung Kim"
date: '2021-03-10'
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    fig_caption: true
    # number_sections: true
    toc: true
    # toc_depth: 2
# header-includes:
#   - \newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}
---

<style>
  p.caption {   <!-- figure caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
  caption {    <!-- table caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
</style>


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      # cache = T,
                      fig.align = "center", fig.width = 12, fig.height = 6)
# Set working directory
knitr::opts_knit$set(root.dir = "../")
```

```{r}
library(GA)   # persp plot
library(mvtnorm)
library(fdapace)   # 1, 2
library(mcfda)   # 7
library(synfd)   # 7
library(doParallel)   # parallel computing
library(doRNG)   # set.seed for foreach
library(MASS)   # huber, rlm
library(kableExtra)
library(tidyverse)
library(gridExtra)
library(latex2exp)
source("functions.R")
source("utills.R")
```


# Iteratively re-weighted least squares (IRLS; IRWLS)

## How to find the solution for robust regression
- The **M-estimator** for robust regression can be obtained from 
  $$
  \hat{\beta} = \underset{\beta}{\operatorname{\arg\min}} \sum_{i=1}^n \rho \left( y_i - x_i^T\beta \right).
  $$

- The **scale-invariant M-estimator** is the following formula :
  $$
  \hat{\beta} = \arg\min_{\beta} \sum_{i=1}^n \rho \left( \frac{y_i - x_i^T\beta}{s} \right)
  $$
  where $s$ is a measure of the scale, and one of the estimate of $s$ is
  $$
  \hat{s} = 1.4826*\text{MAD}\left( \{ |y_i - x_i^T\beta| :i=1,\dots,n\} \right).
  $$

<!-- - Our objective is to find $\hat{\beta} = \arg\min_{\beta} \sum_{i=1}^n \rho \left( \frac{y_i - x_i^T\beta}{s} \right)$, and let $L = \sum_{i=1}^n \rho \left( \frac{y_i - x_i^T\beta}{s} \right)$ where $s = 1.4826*\text{MAD}(\{ |y_i - x_i^T\beta| :i=1,\dots,n\})$. -->
- Now, let $L = \sum_{i=1}^n \rho \left( \frac{y_i - x_i^T\beta}{s} \right)$, then a normal equation could be found by applying 1st derivative : 
  $$
  \frac{\partial L}{\partial \beta} = \sum_{i=1}^n x_i \psi \left( \frac{y_i - x_i^T\beta}{s} \right) = 0, \text{ where } \psi(x) = \rho'(x)
  $$
- Set a weight $w_i = \psi \left( \frac{y_i - x_i^T\beta}{s} \right) / \left( y_i - x_i^T\beta \right)$, then a normal equation could be represented as 
  $$
  \sum_{i=1}^n x_iw_i(y_i - x_i^T\beta) = 0 \\
  \Leftrightarrow X^TWY-XWX^T\beta=0 
  $$
- Therefore, the solution can be obtained by weighted least squares (WLS) as 
  $$ \hat{\beta} = (X^TWX)^{-1}X^TWY $$
- To solve the above equation, we need to compute iteratively by following **Iteratively re-weighted least squares (IRLS) algorithm**.


## IRLS algorithm
1. Set initial value $\hat{\beta}^{(0)}$, then we can compute $W^{(0)}$.
2.  Therefore, $\hat{\beta}^{(1)}$ for 1st iteration can be obtained as
  $$
  \hat{\beta}^{(1)} = (X^TW^{(0)}X)^{-1}X^TW^{(0)}Y.
  $$
3. Similarly, $\hat{\beta}^{(t+1)}$ for $t$th iterations can be represented as
  $$
  \hat{\beta}^{(t+1)} = (X^TW^{(t)}X)^{-1}X^TW^{(t)}Y.
  $$
4. Repeat this procedure until $\hat{\beta}^{(t)}$ is converged.

<br>


## IRLS for local kernel smoother with robust loss function
- The **M-estimator** of local polynomial smoother can be obtained from
  $$
  (\hat{b}_0, \hat{b}_1) = \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \rho \left\{ Y_{ij} - b_0 - b_1(T_{ij}-t) \right\}.
  $$
- The **scale-invariant M-estimator** is the following formula :
  $$
  (\hat{b}_0, \hat{b}_1) = \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \rho \left\{ \frac{Y_{ij} - b_0 - b_1(T_{ij}-t)}{s} \right\}.
  $$
<!-- $$ -->
<!-- \begin{align} -->
<!--   (\hat{b}_0, \hat{b}_1) &= \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \rho \left\{ Y_{ij} - b_0 - b_1(T_{ij}-t) \right\} , \\ -->
<!--   (\hat{b}_0, \hat{b}_1) &= \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\sigma}} (T_{ij}-t) \rho \left[ \{Y_{ij} - \hat{\mu}(T_{ij})\}^2 - b_0 - b_1(T_{ij}-t) \right], -->
<!-- \end{align} -->
<!-- $$ -->

- Let $x_{ij} = (1, T_{ij}-t)^T$ and $\beta = (b_0, b_1)^T$.
- Let $L = \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \rho \left( \frac{Y_{ij} - x_{ij}^T \beta}{s} \right)$, then we could find the solution $\hat{\beta} = \arg\min_{\beta} L$.
- Normal equation :
  $$
  \frac{\partial L}{\partial \beta} = \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) x_{ij} \psi \left( Y_{ij} - x_{ij}^T \beta \right) = 0
  $$
- Set $w_i = \left\{ w_i K_{h_{\mu}} (T_{ij}-t) \psi \left( \frac{Y_{ij} - x_{ij}^T \beta}{s} \right) \right\} / \left( Y_{ij} - x_{ij}^T \beta \right).$
- Apply **IRLS algorithm** until $\hat{\beta}$ is converged.


<!-- - Vectorise $\left( p = 1, \dots, P=\sum_{i=1}^n m_i \right)$ and let weight $K_p = w_i K_{h_{\mu}} (T_{ij}-t)$. -->
<!-- - Let $L = \sum_{p=1}^P K_p \rho \left( \frac{y_p - x_p^T\beta}{s} \right)$, then we could find the solution $\hat{\beta} = \arg\min_{\beta} L$. -->
<!-- - Normal equation : -->
<!--   $$ -->
<!--   \frac{\partial L}{\partial \beta} = \sum_{p=1}^P K_p x_p \psi \left( \frac{y_p - x_p^T\beta}{s} \right) = 0 -->
<!--   $$ -->
<!-- - Set $w_p = \left\{ K_p \psi \left( \frac{y_p - x_p^T\beta}{s} \right) \right\} / \left( y_p - x_p^T\beta \right).$ -->
<!-- - Apply **IRLS algorithm** until $\hat{\beta}$ is converged. -->


<br>

# Selection $\delta$ for Huber loss
## Naive method

$$
  \delta = 1 / \max\left\{ |Y_{ij}| : i = 1,\dots,n, j = 1,\dots,m_i \right\}
$$

## Cross-validation

## Adaptive selection


<br>

# Variance trajectories between different robustness parameters in Huber function and bandwidths
- Huber function의 cutoff parameter $\delta$에 따라 형태가 매우 달라짐.
- 전체적으로 bandwidth는 smoothness에 영향을 주고, trajectory의 형태에는 큰 영향을 미치지는 않음.
- WRM은 Huber loss를 사용한 경우에 비해 under estimate되는 경향이 보임.

## 1st simulation data
```{r data_load_1st, fig.cap = "Figure 1. Sample curves for 1st simulation data."}
### Load example data
load("RData/sim3-2_20210204.RData")
# sim <- 20
sim <- 1
model.cov <- 2   # covariance function setting of the paper (1, 2)
kernel <- "gauss"
bw <- 0.1

# Get simulation data
x <- data.list.outlier[[sim]]$x
gr <- data.list.outlier[[sim]]$gr
# range(unlist(x$t))

### Covariance estimation
work.grid <- seq(min(gr), max(gr), length.out = 51)
cov.true <- get_cov_fragm(gr, model = model.cov)   # true covariance
cov.true <- ConvertSupport(fromGrid = gr, toGrid = work.grid, Cov = cov.true)   # transform to the observed grid

x.2 <- list(Ly = x$y,
            Lt = x$t)

# trajectories of generated data
df <- data.frame(id = factor(unlist(sapply(1:length(x.2$Lt), 
                                           function(id) { 
                                             rep(id, length(x.2$Lt[[id]])) 
                                           }) 
                                    )),
                 t = unlist(x.2$Lt),
                 y = unlist(x.2$Ly))
p1 <- ggplot(df, aes(t, y, color = id)) +
  geom_line(size = 1) +
  labs(x = TeX("$t$"), y = TeX("$Y(t)$")) +
  theme_bw() +
  # ylim(-10, 10) +
  theme(legend.position = "none")
p2 <- p1 + 
  ylim(-10, 10) +
  theme(legend.position = "none")
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```

### $\delta$ in Huber function
```{r huber_delta_1st, fig.cap = "Figure 2. Trajectories of true and estimated variances with different robustness parameter $\\delta$."}
load("RData/20210310_fig_1st.RData")
p1 <- ggplot(df_delta, 
             aes(t, y, color = k_huber, linetype = k_huber)) +
  geom_line(size = 1) +
  scale_linetype_manual(breaks = c("True", paste0("Naive: ", round(k_cand[1], 4)), 
                                   round(k_cand[-1], 4), "Yao", "Lin"),
                        values = c("solid", rep("dashed", 11), rep("solid", 2))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$")) +
  theme_bw() +
  theme(legend.position = c(0.8, 0.65),
        legend.title = element_blank())
p2 <- p1 + 
  ylim(0, 5) +
  theme(legend.position = "none")
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```

### Bandwidth for local kernel smoother and WRM
```{r fig_bw_1st, fig.cap = "Figure 3. Trajectories of true and estimated variances with different bandwidths."}
p1 <- ggplot(df_bw, aes(t, y, color = bw, linetype = bw)) +
  geom_line(size = 1) +
  scale_linetype_manual(breaks = c("True", round(bw_cand, 3)),
                        values = c("solid", rep("dashed", 10))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "Huber loss") +
  theme_bw() +
  theme(legend.position = c(0.5, 0.75),
        legend.title = element_blank())
p2 <- ggplot(df_bw_wrm, aes(t, y, color = bw, linetype = bw)) +
  geom_line(size = 1) +
   scale_linetype_manual(breaks = c("True", round(bw_cand, 3)),
                        values = c("solid", rep("dashed", 10))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "WRM") +
  theme_bw() +
  theme(legend.position = "none",
        legend.title = element_blank())
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```



## 20th simulation data
### $\delta$ in Huber function
```{r data_load_20th, fig.cap = "Figure 4. Sample curves for 20th simulation data and trajectories of true and estimated variances with different robustness parameter $\\delta$."}
### Load example data
load("RData/sim3-2_20210204.RData")
sim <- 20
# sim <- 1
model.cov <- 2   # covariance function setting of the paper (1, 2)
kernel <- "gauss"
bw <- 0.1

# Get simulation data
x <- data.list.outlier[[sim]]$x
gr <- data.list.outlier[[sim]]$gr
# range(unlist(x$t))

### Covariance estimation
work.grid <- seq(min(gr), max(gr), length.out = 51)
cov.true <- get_cov_fragm(gr, model = model.cov)   # true covariance
cov.true <- ConvertSupport(fromGrid = gr, toGrid = work.grid, Cov = cov.true)   # transform to the observed grid

x.2 <- list(Ly = x$y,
            Lt = x$t)

# trajectories of generated data
df <- data.frame(id = factor(unlist(sapply(1:length(x.2$Lt), 
                                           function(id) { 
                                             rep(id, length(x.2$Lt[[id]])) 
                                           }) 
                                    )),
                 t = unlist(x.2$Lt),
                 y = unlist(x.2$Ly))
p1 <- ggplot(df, aes(t, y, color = id)) +
  geom_line(size = 1) +
  labs(x = TeX("$t$"), y = TeX("$Y(t)$")) +
  theme_bw() +
  # ylim(-10, 10) +
  theme(legend.position = "none")

load("RData/20210310_fig_20th.RData")
p2 <- ggplot(df_delta, 
             aes(t, y, color = k_huber, linetype = k_huber)) +
  geom_line(size = 1) +
  scale_linetype_manual(breaks = c("True", paste0("Naive: ", round(k_cand[1], 4)), 
                                   round(k_cand[-1], 4), "Yao", "Lin"),
                        values = c("solid", rep("dashed", 11), rep("solid", 2))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$")) +
  ylim(0, 5) +
  theme_bw() +
  theme(legend.position = c(0.8, 0.65),
        legend.title = element_blank())
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```

<!-- ```{r huber_delta_20nd, fig.cap = "Figure 5. Trajectories of true and estimated variances with different robustness parameter $\\delta$."} -->
<!-- load("RData/20210310_fig_20nd.RData") -->
<!-- p1 <- ggplot(df_delta,  -->
<!--              aes(t, y, color = k_huber, linetype = k_huber)) + -->
<!--   geom_line(size = 1) + -->
<!--   scale_linetype_manual(breaks = c("True", paste0("Naive: ", round(k_cand[1], 4)),  -->
<!--                                    round(k_cand[-1], 4), "Yao", "Lin"), -->
<!--                         values = c("solid", rep("dashed", 11), rep("solid", 2))) + -->
<!--   labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$")) + -->
<!--   theme_bw() + -->
<!--   theme(legend.position = c(0.8, 0.65), -->
<!--         legend.title = element_blank()) -->
<!-- p2 <- p1 +  -->
<!--   ylim(0, 5) + -->
<!--   theme(legend.position = "none") -->
<!-- gridExtra::grid.arrange(p1, p2,  -->
<!--                         nrow = 1) -->
<!-- ``` -->

### Bandwidth for local kernel smoother and WRM
```{r fig_bw_20th, fig.cap = "Figure 5. Trajectories of true and estimated variances with different bandwidths."}
p1 <- ggplot(df_bw, aes(t, y, color = bw, linetype = bw)) +
  geom_line(size = 1) +
  scale_linetype_manual(breaks = c("True", round(bw_cand, 3)),
                        values = c("solid", rep("dashed", 10))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "Huber loss") +
  theme_bw() +
  theme(legend.position = c(0.5, 0.75),
        legend.title = element_blank())
p2 <- ggplot(df_bw_wrm, aes(t, y, color = bw, linetype = bw)) +
  geom_line(size = 1) +
   scale_linetype_manual(breaks = c("True", round(bw_cand, 3)),
                        values = c("solid", rep("dashed", 10))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "WRM") +
  theme_bw() +
  theme(legend.position = "none",
        legend.title = element_blank())
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```






