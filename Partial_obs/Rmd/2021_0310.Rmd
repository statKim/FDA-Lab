---
title: "Partially observed functional data"
author: "Hyunsung Kim"
date: '2021-03-10'
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    fig_caption: true
    # number_sections: true
    toc: true
    # toc_depth: 2
# header-includes:
#   - \newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}
---

<style>
  p.caption {   <!-- figure caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
  caption {    <!-- table caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
</style>


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      # cache = T,
                      fig.align = "center", fig.width = 12, fig.height = 6)
# Set working directory
knitr::opts_knit$set(root.dir = "../")
```

```{r}
library(GA)   # persp plot
library(mvtnorm)
library(fdapace)   # 1, 2
library(mcfda)   # 7
library(synfd)   # 7
library(doParallel)   # parallel computing
library(doRNG)   # set.seed for foreach
library(MASS)   # huber, rlm
library(kableExtra)
library(tidyverse)
library(gridExtra)
library(latex2exp)
source("functions.R")
source("utills.R")
```


# Iteratively re-weighted least squares (IRLS; IRWLS)

## How to find the solution for robust regression
- The **M-estimator** for robust regression can be obtained from 
  $$
  \hat{\beta} = \underset{\beta}{\operatorname{\arg\min}} \sum_{i=1}^n \rho \left( y_i - x_i^T\beta \right).
  $$

- The **scale-invariant M-estimator** is the following formula :
  $$
  \hat{\beta} = \arg\min_{\beta} \sum_{i=1}^n \rho \left( \frac{y_i - x_i^T\beta}{s} \right)
  $$
  where $s$ is a measure of the scale, and one of the estimate of $s$ is
  $$
  \hat{s} = 1.4826*\text{MAD}\left( \{ |y_i - x_i^T\beta| :i=1,\dots,n\} \right).
  $$

<!-- - Our objective is to find $\hat{\beta} = \arg\min_{\beta} \sum_{i=1}^n \rho \left( \frac{y_i - x_i^T\beta}{s} \right)$, and let $L = \sum_{i=1}^n \rho \left( \frac{y_i - x_i^T\beta}{s} \right)$ where $s = 1.4826*\text{MAD}(\{ |y_i - x_i^T\beta| :i=1,\dots,n\})$. -->
- Now, let $L = \sum_{i=1}^n \rho \left( \frac{y_i - x_i^T\beta}{s} \right)$, then a normal equation could be found by applying 1st derivative : 
  $$
  \frac{\partial L}{\partial \beta} = \sum_{i=1}^n x_i \psi \left( \frac{y_i - x_i^T\beta}{s} \right) = 0, \text{ where } \psi(x) = \rho'(x)
  $$
- Set a weight $w_i = \psi \left( \frac{y_i - x_i^T\beta}{s} \right) / \left( y_i - x_i^T\beta \right)$, then a normal equation could be represented as 
  $$
  \sum_{i=1}^n x_iw_i(y_i - x_i^T\beta) = 0 \\
  \Leftrightarrow X^TWY-XWX^T\beta=0 
  $$
- Therefore, the solution can be obtained by weighted least squares (WLS) as 
  $$ \hat{\beta} = (X^TWX)^{-1}X^TWY $$
- To solve the above equation, we need to compute iteratively by following **Iteratively re-weighted least squares (IRLS) algorithm**.


## IRLS algorithm
1. Set initial value $\hat{\beta}^{(0)}$, then we can compute $W^{(0)}$.
2.  Therefore, $\hat{\beta}^{(1)}$ for 1st iteration can be obtained as
  $$
  \hat{\beta}^{(1)} = (X^TW^{(0)}X)^{-1}X^TW^{(0)}Y.
  $$
3. Similarly, $\hat{\beta}^{(t+1)}$ for $t$th iterations can be represented as
  $$
  \hat{\beta}^{(t+1)} = (X^TW^{(t)}X)^{-1}X^TW^{(t)}Y.
  $$
4. Repeat this procedure until $\hat{\beta}^{(t)}$ is converged.

<br>


## IRLS for local kernel smoother with robust loss function
- The **M-estimator** of local polynomial smoother can be obtained from
  $$
  (\hat{b}_0, \hat{b}_1) = \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \rho \left\{ Y_{ij} - b_0 - b_1(T_{ij}-t) \right\}.
  $$
- The **scale-invariant M-estimator** is the following formula :
  $$
  (\hat{b}_0, \hat{b}_1) = \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \rho \left\{ \frac{Y_{ij} - b_0 - b_1(T_{ij}-t)}{s} \right\}.
  $$
<!-- $$ -->
<!-- \begin{align} -->
<!--   (\hat{b}_0, \hat{b}_1) &= \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \rho \left\{ Y_{ij} - b_0 - b_1(T_{ij}-t) \right\} , \\ -->
<!--   (\hat{b}_0, \hat{b}_1) &= \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\sigma}} (T_{ij}-t) \rho \left[ \{Y_{ij} - \hat{\mu}(T_{ij})\}^2 - b_0 - b_1(T_{ij}-t) \right], -->
<!-- \end{align} -->
<!-- $$ -->

- Let $x_{ij} = (1, T_{ij}-t)^T$ and $\beta = (b_0, b_1)^T$.
- Let $L = \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \rho \left( \frac{Y_{ij} - x_{ij}^T \beta}{s} \right)$, then we could find the solution $\hat{\beta} = \arg\min_{\beta} L$.
- Normal equation :
  $$
  \frac{\partial L}{\partial \beta} = \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) x_{ij} \psi \left( Y_{ij} - x_{ij}^T \beta \right) = 0
  $$
- Set $w_i = \left\{ w_i K_{h_{\mu}} (T_{ij}-t) \psi \left( \frac{Y_{ij} - x_{ij}^T \beta}{s} \right) \right\} / \left( Y_{ij} - x_{ij}^T \beta \right).$
- Apply **IRLS algorithm** until $\hat{\beta}$ is converged.


<!-- - Vectorise $\left( p = 1, \dots, P=\sum_{i=1}^n m_i \right)$ and let weight $K_p = w_i K_{h_{\mu}} (T_{ij}-t)$. -->
<!-- - Let $L = \sum_{p=1}^P K_p \rho \left( \frac{y_p - x_p^T\beta}{s} \right)$, then we could find the solution $\hat{\beta} = \arg\min_{\beta} L$. -->
<!-- - Normal equation : -->
<!--   $$ -->
<!--   \frac{\partial L}{\partial \beta} = \sum_{p=1}^P K_p x_p \psi \left( \frac{y_p - x_p^T\beta}{s} \right) = 0 -->
<!--   $$ -->
<!-- - Set $w_p = \left\{ K_p \psi \left( \frac{y_p - x_p^T\beta}{s} \right) \right\} / \left( y_p - x_p^T\beta \right).$ -->
<!-- - Apply **IRLS algorithm** until $\hat{\beta}$ is converged. -->


<br>

# Selection $\delta$ for Huber loss
$$
  \rho(x) = 
    \begin{cases}
      \frac{x^2}{2}, & \mbox{if } |x| \le \delta \\
      \delta(|x|-\frac{1}{2}\delta), & \mbox{if } |x| > \delta
    \end{cases}
$$

- Huber function의 robustness parameter $\delta$에 영향을 크게 받음. ([Figure 1](#delta-in-huber-function), [Figure 3](#delta-in-huber-function-1) 참고)
- **Cross-validation**이나 **data-adaptive method**를 고려해야할 것으로 보임.

## Naive method
$$
  \delta = 1 / \max\left\{ |Y_{ij}| : i = 1,\dots,n, j = 1,\dots,m_i \right\}
$$

## Cross-validation
- CV error 계산에서는 Huber loss 보다는 $L_1$ loss를 사용하는 것이 나을 것으로 생각됨. (여기서도 $\delta$에 depend하기 때문)

## Data-adaptive method
> - Sun, Q., Zhou, W. X., & Fan, J. (2020). [Adaptive huber regression](https://doi.org/10.1080/01621459.2018.1543124). Journal of the American Statistical Association, 115(529), 254-265.
> - Wang, L., Zheng, C., Zhou, W., & Zhou, W. X. (2020). [A new principle for tuning-free Huber regression](https://eprints.soton.ac.uk/441508/1/SS_2019_0045_na.pdf). Statistica Sinica.



<br>

# Variance trajectories between different robustness parameters in Huber function and bandwidths
- Estimated variance $\hat{\sigma}_X^2$은 median based estimator $\hat{\sigma}_0^2$를 모두 빼준 값임.
- Huber function의 cutoff parameter $\delta$에 따라 형태가 매우 달라짐.
- 전체적으로 bandwidth는 smoothness에만 영향을 주고, trajectory의 형태에는 큰 영향을 미치지는 않음.
- WRM은 Huber loss를 사용한 경우에 비해 under estimate되는 경향이 보임.

## 1st simulation data
```{r data_load_1st, fig.cap = "Figure 1. Sample curves for 1st simulation data.", fig.height = 5}
### Load example data
load("RData/sim3-2_20210204.RData")
# sim <- 20
sim <- 1
model.cov <- 2   # covariance function setting of the paper (1, 2)
kernel <- "gauss"
bw <- 0.1

# Get simulation data
x <- data.list.outlier[[sim]]$x
gr <- data.list.outlier[[sim]]$gr
# range(unlist(x$t))

### Covariance estimation
work.grid <- seq(min(gr), max(gr), length.out = 51)
cov.true <- get_cov_fragm(gr, model = model.cov)   # true covariance
cov.true <- ConvertSupport(fromGrid = gr, toGrid = work.grid, Cov = cov.true)   # transform to the observed grid

x.2 <- list(Ly = x$y,
            Lt = x$t)

# trajectories of generated data
df <- data.frame(id = factor(unlist(sapply(1:length(x.2$Lt), 
                                           function(id) { 
                                             rep(id, length(x.2$Lt[[id]])) 
                                           }) 
                                    )),
                 t = unlist(x.2$Lt),
                 y = unlist(x.2$Ly))
p1 <- ggplot(df, aes(t, y, color = id)) +
  geom_line(size = 1) +
  labs(x = TeX("$t$"), y = TeX("$Y(t)$")) +
  theme_bw() +
  # ylim(-10, 10) +
  theme(legend.position = "none")
p2 <- p1 + 
  ylim(-10, 10) +
  theme(legend.position = "none")
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```

### $\delta$ in Huber function
```{r huber_delta_1st, fig.cap = "Figure 2. Trajectories of true and estimated variances with different robustness parameter $\\delta$."}
load("RData/20210310_fig_1st.RData")
p1 <- ggplot(df_delta, 
             aes(t, y, color = k_huber, linetype = k_huber)) +
  geom_line(size = 1) +
  scale_linetype_manual(breaks = c("True", paste0("Naive: ", round(k_cand[1], 4)), 
                                   round(k_cand[-1], 4), "Yao", "Lin"),
                        values = c("solid", rep("dashed", 11), rep("solid", 2))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$")) +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.title = element_blank())
p2 <- p1 + 
  ylim(0, 5) +
  theme(legend.position = "bottom")
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```

### Bandwidth for local kernel smoother and WRM with $\delta = 0.0009$
```{r fig_bw_1st, fig.cap = "Figure 3. Trajectories of true and estimated variances with different bandwidths."}
p1 <- ggplot(df_bw, aes(t, y, color = bw, linetype = bw)) +
  geom_line(size = 1) +
  scale_linetype_manual(breaks = c("True", round(bw_cand, 3)),
                        values = c("solid", rep("dashed", 10))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "Huber loss") +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.title = element_blank())
p2 <- ggplot(df_bw_wrm, aes(t, y, color = bw, linetype = bw)) +
  geom_line(size = 1) +
   scale_linetype_manual(breaks = c("True", round(bw_cand, 3)),
                        values = c("solid", rep("dashed", 10))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "WRM") +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.title = element_blank())
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```



## 20th simulation data
### $\delta$ in Huber function
```{r data_load_20th, fig.cap = "Figure 4. Sample curves for 20th simulation data and trajectories of true and estimated variances with different robustness parameter $\\delta$."}
### Load example data
load("RData/sim3-2_20210204.RData")
sim <- 20
# sim <- 1
model.cov <- 2   # covariance function setting of the paper (1, 2)
kernel <- "gauss"
bw <- 0.1

# Get simulation data
x <- data.list.outlier[[sim]]$x
gr <- data.list.outlier[[sim]]$gr
# range(unlist(x$t))

### Covariance estimation
work.grid <- seq(min(gr), max(gr), length.out = 51)
cov.true <- get_cov_fragm(gr, model = model.cov)   # true covariance
cov.true <- ConvertSupport(fromGrid = gr, toGrid = work.grid, Cov = cov.true)   # transform to the observed grid

x.2 <- list(Ly = x$y,
            Lt = x$t)

# trajectories of generated data
df <- data.frame(id = factor(unlist(sapply(1:length(x.2$Lt), 
                                           function(id) { 
                                             rep(id, length(x.2$Lt[[id]])) 
                                           }) 
                                    )),
                 t = unlist(x.2$Lt),
                 y = unlist(x.2$Ly))
p1 <- ggplot(df, aes(t, y, color = id)) +
  geom_line(size = 1) +
  labs(x = TeX("$t$"), y = TeX("$Y(t)$")) +
  theme_bw() +
  # ylim(-10, 10) +
  theme(legend.position = "none")

load("RData/20210310_fig_20th.RData")
p2 <- ggplot(df_delta, 
             aes(t, y, color = k_huber, linetype = k_huber)) +
  geom_line(size = 1) +
  scale_linetype_manual(breaks = c("True", paste0("Naive: ", round(k_cand[1], 4)), 
                                   round(k_cand[-1], 4), "Yao", "Lin"),
                        values = c("solid", rep("dashed", 11), rep("solid", 2))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$")) +
  ylim(0, 5) +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.title = element_blank())
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```


### Bandwidth for local kernel smoother and WRM with $\delta = 2.5809$
```{r fig_bw_20th, fig.cap = "Figure 5. Trajectories of true and estimated variances with different bandwidths."}
p1 <- ggplot(df_bw, aes(t, y, color = bw, linetype = bw)) +
  geom_line(size = 1) +
  scale_linetype_manual(breaks = c("True", round(bw_cand, 3)),
                        values = c("solid", rep("dashed", 10))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "Huber loss") +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.title = element_blank())
p2 <- ggplot(df_bw_wrm, aes(t, y, color = bw, linetype = bw)) +
  geom_line(size = 1) +
   scale_linetype_manual(breaks = c("True", round(bw_cand, 3)),
                        values = c("solid", rep("dashed", 10))) +
  labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "WRM") +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.title = element_blank())
gridExtra::grid.arrange(p1, p2, 
                        nrow = 1)
```




<!-- <br> -->

<!-- # Results for fixed $\delta = 1.345$ and bandwidth = 0.1 with outlier 2 -->
<!-- ## Covariance estimation -->

<!-- ```{r fig_cov_surf, fig.cap = "Figure 6. True covariance surface and estimated covariance surfaces.", fig.height = 6, fig.width = 8} -->
<!-- par(mfrow = c(4, 4), -->
<!--     mar = c(2, 2, 2, 2)) -->
<!-- load("RData/20210310_outlier_2.RData") -->
<!-- for (sim in 1:4) { -->
<!--   if (sim == 1) { -->
<!--     title <- TRUE -->
<!--   } else { -->
<!--     title <- FALSE -->
<!--   } -->
<!--   plot_cov_surf(cov.est, sim, title = title, lab = paste0("Simulation ", sim)) -->
<!-- } -->
<!-- ``` -->



<!-- ```{r tab_cov_rmise} -->
<!-- ############################# -->
<!-- ### Calculate RMISE with outliers -->
<!-- ############################# -->
<!-- cname <- c("Yao (2005)","Lin (2020)","Huber","WRM") -->
<!-- # res.mat <- matrix(NA, 4, 9) -->
<!-- res.mat <- matrix(NA, 1, 9) -->
<!-- colnames(res.mat) <- c("$N$", cname, cname) -->
<!-- # row.names(res.mat) <- c("Outlier X", -->
<!-- #                         paste0("Outlier ", 1:3)) -->
<!-- row.names(res.mat) <- "Outlier 2" -->
<!-- res.mat2 <- res.mat -->

<!-- for (k in 1:1) { -->
<!--   num.sim <- length(cov.est)   # number of simulations -->

<!--   ### variance -->
<!--   ise.var <- summary_ise(data.list, cov.est, method = "var") -->
<!--   ### covariance -->
<!--   ise.cov <- summary_ise(data.list, cov.est, method = "cov") -->

<!--   # res.mat[k+1, ] <- c(num.sim, -->
<!--   #                     sqrt(rowMeans(ise.var)), -->
<!--   #                     sqrt(rowMeans(ise.cov))) -->
<!--   res.mat[k, ] <- c(num.sim, -->
<!--                       paste0(round(sqrt(rowMeans(ise.var)), 2), -->
<!--                              "(", -->
<!--                              round(apply(ise.var, 1, sd), 2), -->
<!--                              ")"), -->
<!--                       paste0(round(sqrt(rowMeans(ise.cov)), 2), -->
<!--                              "(", -->
<!--                              round(apply(ise.cov, 1, sd), 2), -->
<!--                              ")")) -->


<!--   ### Intrapolation parts (D_0) -->
<!--   ise.intra <- summary_ise(data.list, cov.est, method = "intra") -->
<!--   ### Extrapolation parts (S_0 \ D_0) -->
<!--   ise.extra <- summary_ise(data.list, cov.est, method = "extra") -->

<!--   # res.mat2[k+1, ] <- c(num.sim, -->
<!--   #                      rowMeans(ise.intra), -->
<!--   #                      rowMeans(ise.extra)) -->
<!--   res.mat2[k, ] <- c(num.sim, -->
<!--                       paste0(round(rowMeans(ise.intra), 2), -->
<!--                              "(", -->
<!--                              round(apply(ise.intra, 1, sd), 2), -->
<!--                              ")"), -->
<!--                       paste0(round(rowMeans(ise.extra), 2), -->
<!--                              "(", -->
<!--                              round(apply(ise.extra, 1, sd), 2), -->
<!--                              ")")) -->

<!-- } -->

<!-- knitr::kable(res.mat, digits = 3, align = "r", caption = "Table 1. Average RMISE (standard error) of variances($\\hat{\\sigma}_X^2$) and covariances($\\hat{\\mathbf{C}}$) estimation.") %>% -->
<!--     kable_styling("striped", full_width = FALSE, font_size = 14) %>% -->
<!--     add_header_above(c(" " = 1, " " = 1, "$\\hat{\\sigma}_X^2$" = 4, "$\\hat{\\mathbf{C}}$" = 4)) -->


<!-- knitr::kable(res.mat2, digits = 3, align = "r", caption = "Table 2. Average MISE (standard error) of covariance estimation between intrapolation($\\mathcal{D}_0$) and extrapolation($\\mathcal{S}_0 \\backslash \\mathcal{D}_0$) parts.") %>% -->
<!--     kable_styling("striped", full_width = FALSE, font_size = 14) %>% -->
<!--     add_header_above(c(" " = 1, " " = 1, "$\\mathcal{D}_0$" = 4, "$\\mathcal{S}_0 \\backslash \\mathcal{D}_0$" = 4)) -->
<!-- ``` -->



<!-- <br> -->

<!-- ## PCA -->

<!-- ### Estimated first 3 eigenfunctions -->
<!-- ```{r fig_eig, fig.cap = "Figure 3. Estimated first 3 eigenfunctions for 1st simulation data.", fig.height = 12, fig.width = 15} -->
<!-- p <- list() -->

<!-- for (sim in 1:4) { -->
<!--   p <- c(p, -->
<!--          ggplot_eig(cov.est, sim, main = paste0("Simulation ", sim))) -->
<!-- } -->

<!-- grid.arrange(grobs = p,   # mget(): get multiple objects -->
<!--              nrow = 4) -->
<!-- ``` -->


<!-- ### MISE for first 3 eigenfunctions -->
<!-- ```{r tab_eig_rmise} -->
<!-- # Parallel computing setting -->
<!-- ncores <- detectCores() - 3 -->
<!-- cl <- makeCluster(ncores) -->
<!-- registerDoParallel(cl) -->

<!-- res <- list() -->

<!-- ### Eigen analysis -->
<!-- num.sim <- length(cov.est) -->
<!-- res[[1]] <- sim_eigen_result(cov.est, num.sim, seed = 1000) -->

<!-- stopCluster(cl) -->

<!-- ### Calculate ISE for 1~3 eigenfunctions -->
<!-- cname <- c("Yao (2005)","Lin (2020)","Huber","WRM") -->
<!-- # res.mat <- matrix(NA, 4, 5) -->
<!-- res.mat <- matrix(NA, 1, 5) -->
<!-- colnames(res.mat) <- c("$N$", cname) -->
<!-- row.names(res.mat) <- "Outlier 2" -->
<!-- # row.names(res.mat) <- c("Outlier X", -->
<!-- #                         paste0("Outlier ", 1:3)) -->

<!-- for (j in 1:1) { -->
<!--   pca.est <- res[[j]] -->
<!--   num.sim <- length(pca.est) -->
<!--   ise <- matrix(NA, num.sim, 4) -->

<!--   for (i in 1:num.sim) { -->
<!--     work.grid <- pca.est[[i]]$work.grid -->

<!--     eig.true <- pca.est[[i]]$true -->
<!--     eig.yao <- pca.est[[i]]$yao -->
<!--     eig.lin <- pca.est[[i]]$lin -->
<!--     eig.huber <- pca.est[[i]]$huber -->
<!--     eig.wrm <- pca.est[[i]]$wrm -->

<!--     # K <- length(which(eig.true$PVE < 0.99)) -->
<!--     K <- 3 -->

<!--     # calculate ISE for k eigenfunctions -->
<!--     ise_eig <- matrix(NA, K, 4) -->
<!--     for (k in 1:K) { -->
<!--       ise_eig[k, ] <- c( -->
<!--         get_ise(eig.true$phi[, k], eig.yao$phi[, k], work.grid), -->
<!--         get_ise(eig.true$phi[, k], eig.lin$phi[, k], work.grid), -->
<!--         get_ise(eig.true$phi[, k], eig.huber$phi[, k], work.grid), -->
<!--         get_ise(eig.true$phi[, k], eig.wrm$phi[, k], work.grid) -->
<!--       ) -->
<!--     } -->

<!--     ise[i, ] <- colSums(ise_eig) -->
<!--   } -->

<!--   res.mat[j, ] <- c( -->
<!--     num.sim, -->
<!--     paste0(round(sqrt(colMeans(ise)), 2), -->
<!--            "(", -->
<!--            round(apply(ise, 2, sd), 2), -->
<!--            ")") -->
<!--   ) -->
<!-- } -->


<!-- knitr::kable(res.mat, digits = 3, align = "r", caption = "Table 2. Average MISE (standard error) of first 3 eigenfunctions.") %>% -->
<!-- kable_styling("striped", full_width = FALSE, font_size = 14) -->
<!-- ``` -->







