---
title: "Partially observed functional data"
author: "Hyunsung Kim"
date: '2021-02-24'
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    fig_caption: true
    # number_sections: true
    toc: true
    # toc_depth: 2
# header-includes:
#   - \newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}
---

<style>
  p.caption {   <!-- figure caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
  caption {    <!-- table caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
</style>


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      # cache = T,
                      fig.align = "center", fig.width = 12, fig.height = 6)
# Set working directory
knitr::opts_knit$set(root.dir = "../")
```

```{r}
library(GA)   # persp plot
library(mvtnorm)
library(fdapace)   # 1, 2
library(mcfda)   # 7
library(synfd)   # 7
library(doParallel)   # parallel computing
library(doRNG)   # set.seed for foreach
library(MASS)   # huber, rlm
source("functions.R")
library(kableExtra)
library(tidyverse)
library(gridExtra)
library(latex2exp)
```



# Lin & Wang (2020) + Huber Loss
## Local polynomial kernel smoothing
- The estimation of $\mu(t)$ and $\sigma_X^2(t)$ be $\hat{b}_0$ from the following minimization criterions:
$$
\begin{align}
  (\hat{b}_0, \hat{b}_1) &= \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \{ Y_{ij} - b_0 - b_1(T_{ij}-t) \}^2, \\
  (\hat{b}_0, \hat{b}_1) &= \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n w_i \sum_{j=1}^{m_i} K_{h_{\sigma}} (T_{ij}-t) \left[ \{Y_{ij} - \hat{\mu}(T_{ij})\}^2 - b_0 - b_1(T_{ij}-t) \right]^2,
\end{align}
$$
where $K$ be a kernel function, $h_{\mu}, h_{\sigma}$ bandwidths for $\mu$ and $\sigma_X^2$, $w$ weight.
- 위 식은 weighted least squares (WLS)로 solution을 찾을 수 있음.


## Local polynomial kernel smoothing with Huber loss
- Non-parametric method for estimating $\mu(t)$ and $\sigma_X^2(t)$
- The estimation of $\mu(t)$ and $\sigma_X^2(t)$ be $\hat{b}_0$ from the following minimization criterions:
$$
\begin{align}
  (\hat{b}_0, \hat{b}_1) &= \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n \rho \left[ w_i \sum_{j=1}^{m_i} K_{h_{\mu}} (T_{ij}-t) \{ Y_{ij} - b_0 - b_1(T_{ij}-t) \} \right], \\
  (\hat{b}_0, \hat{b}_1) &= \underset{(b_0,b_1) \in \mathbb{R}^2}{\operatorname{\arg\min}} \sum_{i=1}^n \rho \left[ w_i \sum_{j=1}^{m_i} K_{h_{\sigma}} (T_{ij}-t) \left[ \{Y_{ij} - \hat{\mu}(T_{ij})\}^2 - b_0 - b_1(T_{ij}-t) \right] \right],
\end{align}
$$
where
$$
\rho(x) = 
\begin{cases}
  \frac{1}{2}x^2, & |a| \le k \\
  k(|x| - \frac{1}{2}k), & |a| > k.
\end{cases}
$$
- 위 식은 iterated re-weighted least squares (IWLS)으로 solution을 찾을 수 있음. (`rlm()` in `MASS`)
- IWLS 적용 전, response와 predictors를 weighted variable로 변환해주어야함. (WLS scheme)
- Huber function에서 $k = 1.345$로 고정.



## Cross validation for optimal bandwidth $h$
- The bandwidths for kernel smoothing, $h_{\mu}, h_{\sigma}$, be selected by $\mathcal{K}$-fold cross validation, which minimizes the following CV errors:
$$
\begin{align}
  \text{CV}(h_{\mu}) &= \sum_{k=1}^{\mathcal{K}} \sum_{i \in \mathcal{P}_k} \sum_{j=1}^{m_i} \left\{ Y_{ij} - \hat{\mu}_{h_{\mu},-k}(T_{ij}) \right\}^2 \\
  \text{CV}(h_{\sigma}) &= \sum_{k=1}^{\mathcal{K}} \sum_{i \in \mathcal{P}_k} \sum_{j=1}^{m_i} \left[ \{ Y_{ij} - \hat{\mu}(T_{ij}) \}^2 - \hat{\sigma}_X^2(T_{ij}) \right]^2 
\end{align}
$$


## Problems
- Huber loss로 대체하여 추정된 variance estimator($\hat{\sigma}_X^2$)는 error variance($\hat{\sigma}_0^2$)를 빼지 않은 값임. (원래 이 값을 빼주어야 정확한 estimator임.)
- Outlier가 포함된 데이터로부터 Lin & Wang (2020) 방법으로 추정된 $\hat{\sigma}_0^2$는 매우 큰 값을 가짐. (이 값을 빼게 되면 variance estimator가 모두 음수가 되어 실직적인 값은 0으로 추정된 것임.)
- 따라서 $\sigma_0^2$ 또한 robust하게 estimate하는 방법을 생각해봐야할 것으로 보임


<br>

# Results for covariance estimation
- CV를 통해 optimal한 bandwidth를 찾아야하지만, 이번 결과에서는 cv error 계산식에서 오류가 발견되어 임의로 선택한 것이라 생각하면 될 것 같습니다.
- Variance estimation에서 `huber()` in `MASS`를 사용한 방법은 local kernel smoothing이 아닌 각 time point에서의 값으로 구한 location estimation이다보니 결과 요약에서 제외함. (Figure 1과 대응되는 그림은 [Appendix](#appendix) 참고)

## Variance estimation
- Yao (2005)의 경우, variance estimator가 음수가 나오기도 함. (함수 내에서 따로 처리해주지 않음)
- Lin (2020)의 경우, `mcfda`의 함수에서 estimator가 음수가 나오면 0으로 처리함.

```{r fig1, fig.cap = "Figure 1. Trajectories of true and estimated variances.", fig.height = 8, fig.width = 15}
p <- list()

sim <- 20
for (k in 1:3) {
  fname <- paste("RData/sim5-", k, "_20210224.RData", sep = "")
  load(fname)
  
  # remove list contating "null"  
  ind <- which(!sapply(cov.est.outlier, is.null))
  num.sim <- length(ind)   # number of simulations
  data.list.outlier <- data.list.outlier[ind]
  cov.est.outlier <- cov.est.outlier[ind]
  
  # Get simulation data
  # x <- data.list.outlier[[sim]]$x
  # gr <- data.list.outlier[[sim]]$gr
  
  work.grid <- cov.est.outlier[[sim]]$work.grid
  cov.list <- cov.est.outlier[[sim]]$cov
  
  # # pointwise huber estimation
  # Lt <- unlist(x$t)
  # Ly <- unlist(x$y)
  # mu.huber.pt <- sapply(gr, function(t) {
  #   ind <- which(Lt == t)
  #   return( huber(Ly[ind], k = 1.345)$mu )
  # })
  # var.huber.pt <- sapply(gr, function(t) {
  #   ind <- which(Lt == t)
  #   ind_mu <- which(gr == t)
  #   var <- tryCatch({
  #     huber((Ly[ind] - mu.huber.pt[ind_mu])^2, k = 1.345)$mu 
  #   }, error = function(e) {
  #     return(0)
  #   })
  #   var <- ifelse(var < 0, 0, var)   # set 0 for negative variances
  #   return(var)
  # })
  
  df <- data.frame(
    x = rep(work.grid, 4),
    y = c(diag(cov.list$true),
          diag(cov.list$yao),
          diag(cov.list$lin),
          diag(cov.list$huber)),
    method = factor(rep(c("True","Yao(2005)","Lin(2020)","Lin + Huber loss"),
                        each = length(work.grid)),
                    levels = c("True","Yao(2005)","Lin(2020)","Lin + Huber loss"))
  )
  p[[2*(k-1)+1]] <- ggplot(df, aes(x, y, group = method, color = method)) +
    geom_line(size = 1) +
    labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = paste("Outlier", k)) +
    geom_hline(yintercept = 0, size = 0.8) +
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.position = "bottom")
  p[[2*(k-1)+2]] <- df %>% 
    filter(method %in% c("True","Lin + Huber loss")) %>% 
    ggplot(aes(x, y, group = method, color = method)) +
    geom_line(size = 1) +
    labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "") +
    geom_hline(yintercept = 0, size = 0.8) +
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.position = "bottom")
}

grid.arrange(grobs = p,   # mget(): get multiple objects
             nrow = 2,
             as.table = F)   # columnwise plotting
```

## Covariance estimation

```{r fig2, fig.cap = "Figure 2. True covariance surface and estimated covariance surfaces.", fig.height = 6, fig.width = 8}
# sim <- 20
par(mfrow = c(3, 4),
    mar = c(2, 2, 2, 2))
for (k in 1:3) {
  fname <- paste("RData/sim5-", k, "_20210224.RData", sep = "")
  load(fname)
  
  # remove list contating "null"  
  ind <- which(!sapply(cov.est.outlier, is.null))
  data.list.outlier <- data.list.outlier[ind]
  cov.est.outlier <- cov.est.outlier[ind]
  
  if (k == 1) {
    main <- c("True","Yao et al. (2005)","Lin & Wang (2020)","Lin + Huber loss")
  } else {
    main <- c("","","","")
  }
  
  ### Covariance surface
  work.grid <- cov.est.outlier[[sim]]$work.grid
  cov.list <- cov.est.outlier[[sim]]$cov
  # par(mfrow = c(1, 3),
  #     mar = c(0, 2, 7, 2))
  persp3D(work.grid, work.grid, cov.list$true, 
          theta = -70, phi = 30, expand = 1,
          main = main[1], 
          xlab = "s", ylab = "t", zlab = "C(s,t)")
  mtext(paste("Outlier", k), side = 2)
  persp3D(work.grid, work.grid, cov.list$yao, 
          theta = -70, phi = 30, expand = 1,
          main = main[2], 
          xlab = "s", ylab = "t", zlab = "C(s,t)")
  persp3D(work.grid, work.grid, cov.list$lin, 
          theta = -70, phi = 30, expand = 1,
          main = main[3],
          xlab = "s", ylab = "t", zlab = "C(s,t)")
  persp3D(work.grid, work.grid, cov.list$huber, 
          theta = -70, phi = 30, expand = 1,
          main = main[4],
          xlab = "s", ylab = "t", zlab = "C(s,t)")
}
```



```{r tab1}
#############################
### Calculate RMISE with outliers
#############################
cname <- c("Yao (2005)","Lin (2020)","Lin + Huber loss")
res.mat <- matrix(NA, 3, 7)
colnames(res.mat) <- c("$N$", cname, cname)
row.names(res.mat) <- paste0("Outlier ", 1:3)
res.mat2 <- res.mat

for (k in 1:3) {
  fname <- paste("RData/sim5-", k, "_20210224.RData", sep = "")
  load(fname)
  
  # remove list contating "null"  
  ind <- which(!sapply(cov.est.outlier, is.null))
  num.sim <- length(ind)   # number of simulations
  data.list.outlier <- data.list.outlier[ind]
  cov.est.outlier <- cov.est.outlier[ind]
  
  ### variance
  ise.var <- summary_ise(data.list.outlier, cov.est.outlier, method = "var")
  ### covariance
  ise.cov <- summary_ise(data.list.outlier, cov.est.outlier, method = "cov")
  
  # res.mat[k+1, ] <- c(num.sim,
  #                     sqrt(rowMeans(ise.var)),
  #                     sqrt(rowMeans(ise.cov)))
  res.mat[k, ] <- c(num.sim,
                      paste0(round(sqrt(rowMeans(ise.var)), 2), 
                             "(", 
                             round(apply(ise.var, 1, sd), 2), 
                             ")"),
                      paste0(round(sqrt(rowMeans(ise.cov)), 2), 
                             "(", 
                             round(apply(ise.cov, 1, sd), 2), 
                             ")"))
  
  
  ### Intrapolation parts (D_0)
  ise.intra <- summary_ise(data.list.outlier, cov.est.outlier, method = "intra")
  ### Extrapolation parts (S_0 \ D_0)
  ise.extra <- summary_ise(data.list.outlier, cov.est.outlier, method = "extra")

  # res.mat2[k+1, ] <- c(num.sim,
  #                      rowMeans(ise.intra),
  #                      rowMeans(ise.extra))
  res.mat2[k, ] <- c(num.sim,
                      paste0(round(rowMeans(ise.intra), 2), 
                             "(", 
                             round(apply(ise.intra, 1, sd), 2), 
                             ")"),
                      paste0(round(rowMeans(ise.extra), 2), 
                             "(", 
                             round(apply(ise.extra, 1, sd), 2), 
                             ")"))
  # res.mat2[k+1, 3:4] <- rowMeans(ise.extra)   # ISE
  # ise_sd[2, 3:4] <- apply(ise.cov, 1, sd)
}

knitr::kable(res.mat, digits = 3, align = "r", caption = "Table 1. Average RMISE (standard error) of variances($\\hat{\\sigma}_X^2$) and covariances($\\hat{\\mathbf{C}}$) estimation.") %>% 
    kable_styling("striped", full_width = FALSE, font_size = 14) %>% 
    add_header_above(c(" " = 1, " " = 1, "$\\hat{\\sigma}_X^2$" = 3, "$\\hat{\\mathbf{C}}$" = 3))


knitr::kable(res.mat2, digits = 3, align = "r", caption = "Table 2. Average MISE (standard error) of covariance estimation between intrapolation($\\mathcal{D}_0$) and extrapolation($\\mathcal{S}_0 \\backslash \\mathcal{D}_0$) parts.") %>% 
    kable_styling("striped", full_width = FALSE, font_size = 14) %>% 
    add_header_above(c(" " = 1, " " = 1, "$\\mathcal{D}_0$" = 3, "$\\mathcal{S}_0 \\backslash \\mathcal{D}_0$" = 3))
```



<br>

# Results for PCA

## Estimated first 3 eigenfunctions
```{r fig3, fig.cap = "Figure 3. Estimated first 3 eigenfunctions for 1st simulation data.", fig.height = 12, fig.width = 15}
p <- list()

# sim <- 20
### With outliers
for (k in 1:3) {
  fname <- paste0("RData/sim5-", k, "_20210224.RData")
  load(fname)
  
  # remove list contating "null"  
  ind <- which(!sapply(cov.est.outlier, is.null))
  cov.est.outlier <- cov.est.outlier[ind]

  # estimated covariances from Simulation 3
  work.grid <- cov.est.outlier[[sim]]$work.grid
  cov.true <- cov.est.outlier[[sim]]$cov$true
  cov.yao <- cov.est.outlier[[sim]]$cov$yao
  cov.lin <- cov.est.outlier[[sim]]$cov$lin
  cov.huber <- cov.est.outlier[[sim]]$cov$huber  
  
  # eigen analysis
  eig.true <- get_eigen(cov = cov.true, grid = work.grid)
  eig.yao <- get_eigen(cov = cov.yao, grid = work.grid)
  eig.lin <- get_eigen(cov = cov.lin, grid = work.grid)
  eig.huber <- get_eigen(cov = cov.huber, grid = work.grid)
  
  ## change eigen direction(sign) for first K eigenvectors
  # K <- min(ncol(eig.true$phi),
  #          ncol(eig.yao$phi),
  #          ncol(eig.lin$phi),
  #          ncol(eig.huber$phi))
  K <- 3
  eig.yao$phi[, 1:K] <- check_eigen_sign(eig.yao$phi[, 1:K], eig.true$phi[, 1:K])
  eig.lin$phi[, 1:K] <- check_eigen_sign(eig.lin$phi[, 1:K], eig.true$phi[, 1:K])
  eig.huber$phi[, 1:K] <- check_eigen_sign(eig.huber$phi[, 1:K], eig.true$phi[, 1:K])
    
  # fitst 3 eigenfunctions
  for (i in 1:K) {
    if (i == 1) {
      p_title <- paste("Outlier", k)
    } else {
      p_title <- ""
    }
    
    fig.data <- data.frame(work.grid = rep(work.grid, 4),
                           phi = c(eig.true$phi[, i],
                                   eig.yao$phi[, i],
                                   eig.lin$phi[, i],
                                   eig.huber$phi[, i]),
                           method = rep(c("True","Yao (2005)","Lin (2020)","Lin + Huber"), 
                                        each = length(work.grid)))
    p[[3*(k-1)+i]] <- ggplot(data = fig.data, 
                             mapping = aes(work.grid, phi, color = method)) +
      geom_line(size = 1) +
      labs(x = TeX("$t$"), y = TeX(paste0("$\\phi_", i, "(t)$")), title = p_title) +
      scale_color_discrete(breaks = c("True","Yao (2005)","Lin (2020)","Lin + Huber")) +
      theme_bw() +
      theme(legend.title = element_blank(),
            legend.position = "bottom")
  }
}

grid.arrange(grobs = p,   # mget(): get multiple objects
             nrow = 3)
```


## MISE for first 3 eigenfunctions
```{r tab2}
# Parallel computing setting
ncores <- detectCores() - 3
cl <- makeCluster(ncores)
registerDoParallel(cl)

res <- list()

### Eigen analysis
for (k in 1:3) {
  fname <- paste0("RData/sim5-", k, "_20210224.RData")
  load(fname)
  
  # remove list contating "null"  
  ind <- which(!sapply(cov.est.outlier, is.null))
  cov.est.outlier <- cov.est.outlier[ind]
  num.sim <- length(ind)
  
  res[[k]] <- sim_eigen_result(cov.est.outlier, num.sim, seed = 1000)
}
stopCluster(cl)  

### Calculate ISE for 1~3 eigenfunctions
cname <- c("Yao (2005)","Lin (2020)","Lin + Huber loss")
res.mat <- matrix(NA, 3, 4)
colnames(res.mat) <- c("$N$", cname)
row.names(res.mat) <- paste0("Outlier ", 1:3)

for (j in 1:3) {
  pca.est <- res[[j]]
  num.sim <- length(pca.est)
  ise <- matrix(NA, num.sim, 3)
  
  for (i in 1:num.sim) {
    work.grid <- pca.est[[i]]$work.grid
    
    eig.true <- pca.est[[i]]$true
    eig.yao <- pca.est[[i]]$yao
    eig.lin <- pca.est[[i]]$lin
    eig.huber <- pca.est[[i]]$huber
    
    # K <- length(which(eig.true$PVE < 0.99))
    K <- 3

    # calculate ISE for k eigenfunctions
    ise_eig <- matrix(NA, K, 3)
    for (k in 1:K) {
      ise_eig[k, ] <- c(
        get_ise(eig.true$phi[, k], eig.yao$phi[, k], work.grid),
        get_ise(eig.true$phi[, k], eig.lin$phi[, k], work.grid),
        get_ise(eig.true$phi[, k], eig.huber$phi[, k], work.grid)
      )
    }
    
    ise[i, ] <- colSums(ise_eig)
  }
  
  res.mat[j, ] <- c(
    num.sim,
    paste0(round(sqrt(colMeans(ise)), 2), 
           "(", 
           round(apply(ise, 2, sd), 2), 
           ")")
  )
}

knitr::kable(res.mat, digits = 3, align = "r", caption = "Table 2. Average MISE (standard error) of first 3 eigenfunctions.") %>% 
  kable_styling("striped", full_width = FALSE, font_size = 14)
```


<br>

# Appendix

```{r fig4, fig.cap = "Figure 4. Trajectories of true and estimated variances.", fig.height = 8, fig.width = 15}
p <- list()

# sim <- 20
for (k in 1:3) {
  fname <- paste("RData/sim5-", k, "_20210224.RData", sep = "")
  load(fname)
  
  # remove list contating "null"  
  ind <- which(!sapply(cov.est.outlier, is.null))
  num.sim <- length(ind)   # number of simulations
  data.list.outlier <- data.list.outlier[ind]
  cov.est.outlier <- cov.est.outlier[ind]
  
  # Get simulation data
  x <- data.list.outlier[[sim]]$x
  gr <- data.list.outlier[[sim]]$gr
  
  work.grid <- cov.est.outlier[[sim]]$work.grid
  cov.list <- cov.est.outlier[[sim]]$cov
  
  # pointwise huber estimation
  Lt <- unlist(x$t)
  Ly <- unlist(x$y)
  mu.huber.pt <- sapply(gr, function(t) {
    ind <- which(Lt == t)
    return( huber(Ly[ind], k = 1.345)$mu )
  })
  var.huber.pt <- sapply(gr, function(t) {
    ind <- which(Lt == t)
    ind_mu <- which(gr == t)
    var <- tryCatch({
      huber((Ly[ind] - mu.huber.pt[ind_mu])^2, k = 1.345)$mu
    }, error = function(e) {
      return(0)
    })
    var <- ifelse(var < 0, 0, var)   # set 0 for negative variances
    return(var)
  })
  
  df <- data.frame(
    x = rep(work.grid, 5),
    y = c(diag(cov.list$true),
          diag(cov.list$yao),
          diag(cov.list$lin),
          diag(cov.list$huber),
          var.huber.pt),
    method = factor(rep(c("True","Yao(2005)","Lin(2020)","Lin+rlm()","Lin+huber()"),
                        each = length(work.grid)),
                    levels = c("True","Yao(2005)","Lin(2020)","Lin+rlm()","Lin+huber()"))
  )
  p[[2*(k-1)+1]] <- ggplot(df, aes(x, y, group = method, color = method)) +
    geom_line(size = 1) +
    labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = paste("Outlier", k)) +
    geom_hline(yintercept = 0, size = 0.8) +
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.position = "bottom")
  p[[2*(k-1)+2]] <- df %>% 
    filter(method %in% c("True","Lin+rlm()","Lin+huber()")) %>% 
    ggplot(aes(x, y, group = method, color = method)) +
    geom_line(size = 1) +
    labs(x = TeX("$t$"), y = TeX("$\\sigma_X^2(t)$"), title = "") +
    geom_hline(yintercept = 0, size = 0.8) +
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.position = "bottom")
}

grid.arrange(grobs = p,   # mget(): get multiple objects
             nrow = 2,
             as.table = F)   # columnwise plotting
```

