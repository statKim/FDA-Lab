---
title: "Robust PCA for functional snippets"
author: "Hyunsung Kim"
date: '2021-05-13'
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    fig_caption: true
    # number_sections: true
    toc: true
    # toc_depth: 2
# header-includes:
#   - \newcommand{\argmin}{\mathop{\mathrm{argmin}}\limits}
---

<style>
  p.caption {   <!-- figure caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
  caption {    <!-- table caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
</style>


```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
library(tidyverse)
knitr::opts_chunk$set(
  echo = FALSE, message = FALSE, warning = FALSE, 
  # cache = T,
  fig.align = "center", fig.width = 12, fig.height = 6
)
# Set working directory
# knitr::opts_knit$set(root.dir = "../")
```


# Noise variance
- 코드에서 실수가 있었음 (average에서는 성립하지만, median에서는 성립하지 않음)
- estiamte이 크게 달라짐
  - 0.1 => 0.7
  - 0.03 => 0.5
- trimmed mean으로 사용하면 값이 줄어들긴 하지만 어느정도를 trim할지를 결정해야함


<br>

# AMI clustering
- 0이 많은 case를 강제로 추가하여 outlier로 생각하고 clustering
- 나머지 setting은 기존과 동일
- 그다지 결과는 좋지 않음...(0이 많은 case들이 전체적으로 값이 작은 curve들과 같은 그룹으로 묶임)


<br>


# Clustering simulation
- doppler signal 4개를 true cluster라 하고 시행
- normal noise를 주고, outlier의 경우는 기존 outlier의 noise를 줌
- 3 방법론 비교 (Yao, Kraus, Huber)
- 결과
  - Huber의 경우, variance가 0으로 estimate되어 직접적인 비교는 힘들어보임...
  - 초기값에 따라 결과가 많이 달라짐
  - outlier 없는 경우, Kraus가 굉장히 좋음 (4개를 모두 맞히는 경우가 생김)
  - outlier 있는 경우, 3 방법 모두 안좋음
    - ex) 3 / 1 / 94 / 2 
    
<br> 


# M-estimator for partially observed functional data

## Marginal M-estimator for mean (Park et al. (2020))
- 박연주 교수님 arxiv paper에서 언급된 식.

$$
\hat{\mu}^M(t) = \arg\min_{\theta} \sum_{i=1}^n O_i(t) \rho\left( X_i(t) - \theta \right)
$$

## Marginal M-estimator for covariance
- raw covariance 식에서 M-estimator 식으로 바꿔본 식

$$
\hat{\sigma}^M(s,t) = \arg\min_{\theta} \sum_{i=1}^n U_i(s,t) \rho\left( \{X_i(s) - \hat{\mu}^M(s)\} \{X_i(t) - \hat{\mu}^M(t)\} - \theta \right),
$$
where $U_i(s,t) = O_i(s)O_i(t)$.

## Completion result
```{r}
df <- data.frame(
  method = c("Yao (2005)","Huber","M-est","M-est (smooth)","Kraus (2015)","Kraus-M","Kraus-M (smooth)"),
  MISE.x = c("0.62 (2.97)","0.06 (0.03)","0.14 (0.22)","0.05 (0.05)","0.36 (0.09)","0.15 (0.06)","0.05 (0.03)"),
  MSE.x = c("34.82 (160.04)","3.59 (1.87)","8.24 (12.97)","2.75 (2.77)","21.09 (4.72)","9.16 (3.25)","3.14 (1.60)"),
  MISE.y = c("1.83 (5.75)","0.08 (0.07)","0.21 (0.25)","0.06 (0.05)",NA,NA,NA),
  MSE.y = c("97.54 (304.37)","4.64 (3.71)","11.62 (14.13)","3.4 (2.90)",NA,NA,NA)
)

knitr::kable(df,
             # digits = 3,
             col.names = c("Method","MISE","MSE","MISE","MSE"),
             align = "c",
             escape = FALSE,
             caption = "Table 1. MISE and MSE for completion and reconstruction.") %>%
    kable_styling("striped", full_width = FALSE, font_size = 14) %>%
    add_header_above(c(" " = 1,
                       "Completion" = 2,
                       "Reconstruction" = 2))
# method "1"                    "2"           "3"             "4"           "5"            "6"           "7"
# # n.sim = 50, bw = 0.1
# MISE.x "419.34 (2888.5)"      "0.06 (0.03)" "0.15 (0.24)"   "0.06 (0.05)" "0.37 (0.08)"  "0.15 (0.05)" "0.05 (0.02)"
# MSE.x  "21465.46 (147212.4)"  "3.52 (1.57)" "8.55 (13.65)"  "3.12 (2.92)" "21.24 (4.38)" "9.16 (2.63)" "3.22 (1.35)"
# MISE.y "628.67 (4016.48)"     "0.08 (0.05)" "0.22 (0.27)"   "0.07 (0.06)" NA             NA            NA           
# MSE.y  "31758.03 (201631.02)" "4.46 (2.51)" "12.14 (15.02)" "3.84 (3.01)" NA             NA            NA
# # n.sim = 100, bw 0.2
# MISE.x "0.62 (2.97)"    "0.06 (0.03)" "0.14 (0.22)"   "0.06 (0.06)" "0.36 (0.09)"  "0.15 (0.06)" "0.05 (0.03)"
# MSE.x  "34.82 (160.04)" "3.59 (1.87)" "8.24 (12.97)"  "3.58 (3.58)" "21.09 (4.72)" "9.16 (3.25)" "3.16 (1.51)"
# MISE.y "1.83 (5.75)"    "0.08 (0.07)" "0.21 (0.25)"   "0.08 (0.07)" NA             NA            NA           
# MSE.y  "97.54 (304.37)" "4.64 (3.71)" "11.62 (14.13)" "4.28 (3.7)"  NA             NA            NA      

# MISE.x "0.62 (2.97)"    "0.06 (0.03)" "0.14 (0.22)"   "0.05 (0.05)" "0.36 (0.09)"  "0.15 (0.06)" "0.05 (0.03)"
# MSE.x  "34.82 (160.04)" "3.59 (1.87)" "8.24 (12.97)"  "2.75 (2.77)" "21.09 (4.72)" "9.16 (3.25)" "3.14 (1.6)" 
# MISE.y "1.83 (5.75)"    "0.08 (0.07)" "0.21 (0.25)"   "0.06 (0.05)" NA             NA            NA           
# MSE.y  "97.54 (304.37)" "4.64 (3.71)" "11.62 (14.13)" "3.4 (2.9)"   NA             NA            NA        
```

## Completion result (pre-smoothing)
```{r}
df <- data.frame(
  method = c("Yao (2005)","Huber","M-est","M-est (smooth)","Kraus (2015)","Kraus-M","Kraus-M (smooth)"),
  MISE.x = c("365.05 (3607.21)","0.05 (0.03)","0.07 (0.07)","0.41 (1.00)","0.15 (0.09)","0.06 (0.03)","0.12 (0.05)"),
  MSE.x = c("21012.03 (207630.60)","3.08 (1.92)","3.93 (3.91)","22.63 (56.77)","8.59 (5.14)","3.67 (1.72)","7.12 (2.75)"),
  MISE.y = c("516.66 (5057.71)","0.07 (0.07)","0.09 (0.07)","0.42 (1.02)",NA,NA,NA),
  MSE.y = c("27852.52 (273000.81)","4.10 (3.90)","4.82 (4.04)","23.57 (57.75)",NA,NA,NA)
)

knitr::kable(df,
             # digits = 3,
             col.names = c("Method","MISE","MSE","MISE","MSE"),
             align = "c",
             escape = FALSE,
             caption = "Table 2. MISE and MSE for completion and reconstruction.") %>%
    kable_styling("striped", full_width = FALSE, font_size = 14) %>%
    add_header_above(c(" " = 1,
                       "Completion" = 2,
                       "Reconstruction" = 2))
```



<br>

# CV
```{r}
# delta cv
# > ### variance
#   > ise.var <- summary_ise(data.list, cov.est, method = "var")
# > sqrt(rowMeans(ise.var))
# [1] 0.9612889 1.1383390
# > apply(ise.var, 1, sd)
# [1] 0.4071509 0.5122131
# > 
#   > ### covariance
#   > ise.cov <- summary_ise(data.list, cov.est, method = "cov")
# > sqrt(rowMeans(ise.cov))
# [1] 0.5712111 0.6748887
# > apply(ise.cov, 1, sd)
# [1] 0.1624076 0.2043409
# > 
#   > ### Intrapolation parts (D_0)
#   > ise.intra <- summary_ise(data.list, cov.est, method = "intra")
# > rowMeans(ise.intra)
# [1] 0.2526136 0.3829456
# > apply(ise.intra, 1, sd)
# [1] 0.1519010 0.1777714
# > 
#   > ### Extrapolation parts (S_0 \ D_0)
#   > ise.extra <- summary_ise(data.list, cov.est, method = "extra")
# > rowMeans(ise.extra)
# [1] 0.07366862 0.07252921
# > apply(ise.extra, 1, sd)
# [1] 0.04547889 0.03677989

# bw CV
# > ### variance
#   > ise.var <- summary_ise(data.list, cov.est, method = "var")
# > sqrt(rowMeans(ise.var))
# [1] 0.9749018 0.9964617
# > apply(ise.var, 1, sd)
# [1] 0.4268462 0.4214440
# > 
#   > ### covariance
#   > ise.cov <- summary_ise(data.list, cov.est, method = "cov")
# > sqrt(rowMeans(ise.cov))
# [1] 0.5673462 0.5755957
# > apply(ise.cov, 1, sd)
# [1] 0.1556944 0.1928741
# > 
#   > ### Intrapolation parts (D_0)
#   > ise.intra <- summary_ise(data.list, cov.est, method = "intra")
# > rowMeans(ise.intra)
# [1] 0.2499317 0.2718674
# > apply(ise.intra, 1, sd)
# [1] 0.1476513 0.1761322
# > 
#   > ### Extrapolation parts (S_0 \ D_0)
#   > ise.extra <- summary_ise(data.list, cov.est, method = "extra")
# > rowMeans(ise.extra)
# [1] 0.07195002 0.05944308
# > apply(ise.extra, 1, sd)
# [1] 0.04527774 0.04002467

# # delta cv
# > sqrt(colMeans(ise))
# [1] 0.6472023 0.7793626
# > apply(ise, 2, sd)
# [1] 0.5172154 0.7363772

# bw cv
# > sqrt(colMeans(ise))
# [1] 0.6491594 0.5997455
# > apply(ise, 2, sd)
# [1] 0.5171207 0.5646688
```


  

