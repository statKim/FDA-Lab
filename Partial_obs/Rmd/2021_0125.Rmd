---
title: "Partially observed functional data"
author: "Hyunsung Kim"
date: '2021-01-25'
output: 
  prettydoc::html_pretty:
    theme: leonids
    # theme: hpstr
    highlight: github
    fig_caption: true
    # number_sections: true
    toc: true
    # toc_depth: 2
---

<style>
  p.caption {   <!-- figure caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
  caption {    <!-- table caption -->
    font-size: 0.9em;
    font-style: italic;
    color: grey;
    <!-- margin-right: 10%; -->
    <!-- margin-left: 10%;   -->
    text-align: justify;
  }
</style>

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      # cache = T,
                      fig.align = "center", fig.width = 12, fig.height = 6)
# Set working directory
knitr::opts_knit$set(root.dir = "C:/Users/user/Desktop/KHS/FDA-Lab/Partial_obs")
```

# Simulation settings
> Lin, Z., & Wang, J. L. (2020). Mean and covariance estimation for functional snippets. Journal of the American Statistical Association, 1-13.

- 위 논문의 5장에서 $\text{cov I}, n=200, \delta=0.25, \text{SNR}=2$인 경우를 활용하였으며, $N=100$번 반복시행
- 기존 모형과 outlier를 추가하였을 때의 차이를 확인 (outlier 비율은 0.2라 가정)
- Variance $\sigma_X^2$과 covariance $C$의 estimation performance를 비교


## Methods for comparison
1. Yao, Müller, and Wang (2005) : `fdapace`
2. Liu and Müller (2009) : `fdapace`
3. Kraus (2015) : `Kraus(2015)/`
4. Zhang and Chen (2017) : `코드 못찾음`
5. Descary and Panaretos (2019) : `코드 못찾음`
6. Delaigle et al. (2020) : `Delaigle(2020)/` (matlab 코드)
7. Lin & Wang (2020) : [`mcfda`](https://github.com/linulysses/mcfda/)

최종적으로 **1. Yao, Müller, and Wang (2005)**과 **7. Lin & Wang (2020)**만을 비교함.

`Lin & Wang. (2020)` setting(functional snippets)에 적용가능한 방법은 1, 7번만 가능하며, `Kraus (2015)` 방법은 functional snippet case에서 사전 과정인 raw covariance 계산 자체가 불가능.

`Yao et al. (2005)`는 <u>Principal analysis by conditional expecetation(PACE)를 제안하여 PCA와 curve reconstruction 관점에서의 비교</u>를 하였고, `Liu & Müller (2009)`는 <u>PACE를 확장하여 derivative estimation에 대한 방법론</u>을 제시. (즉, 이 두 방법에서는 measurement erros $\sigma_0^2$를 같은 방식으로 estimate하는데, `Lin & Wang (2020)`에서 $\sigma_0^2$에 대한 RMSE를 비교한 것이 적절한 것인지 의문)


## Performance measure
- **Variance estimation**
\begin{aligned}
  \text{RMISE} &= \sqrt{ \frac{1}{N} \sum_{i=1}^N \int_{\mathcal{T}} |\hat{\sigma}^2_X(t) - \sigma^2_X(t)|^2 dt} \\
  \text{RMSE}  &= \sqrt{ \frac{1}{N} \sum_{i=1}^N \frac{1}{n(\mathcal{T})} \sum_{t \in \mathcal{T}} |\hat{\sigma}^2_X(t) - \sigma^2_X(t)|^2}
\end{aligned}

- **Covariance estimation**
\begin{aligned}
  \text{RMISE} &= \sqrt{ \frac{1}{N} \sum_{i=1}^N \int_{\mathcal{T}}\int_{\mathcal{T}} |\hat{C}(s,t) - C(s,t)|^2 dsdt} \\
  \text{RMSE}  &= \sqrt{ \frac{1}{N} \sum_{i=1}^N \frac{1}{\{n(\mathcal{T})\}^2} \sum_{t \in \mathcal{T}} \sum_{s \in \mathcal{T}} |\hat{C}(s,t) - C(s,t)|^2}
\end{aligned}



```{r}
library(GA)   # persp plot
library(kableExtra)
library(fdapace)   # 1, 2
library(mcfda)   # 7
library(synfd)   # 7
source("functions.R")
source("Kraus(2015)/pred.missfd.R")   # 3
source("Kraus(2015)/simul.missfd.R")  # 3

load("RData/20210125.RData")

outlier.ratio <- 0.2   # ratio of outliers
n <- 200   # number of curves
n.outlier <- ceiling(n*outlier.ratio)
```


<br>


# Simulation results

## Original model

```{r fig1, fig.cap = "Figure 1. Design plot (Left) and smaple trajectories with mean curve (Right)."}
#############################
### Visualization
#############################
i <- 1
x <- data.list[[i]]$x

### Design plot and sample trajectories
par(mfrow = c(1, 2))
CreateDesignPlot(x$t, main = "")
plot(x$t[[1]], x$y[[1]], type = "l", col="grey",
     xlim = range(unlist(x$t)), ylim = range(unlist(x$y)), xlab = "Time", ylab = "")
for (j in 2:n) {
  lines(x$t[[j]], x$y[[j]], col = "grey")
}
mu <- function(s) { 2*(s^2)*cos(2*pi*s) }   # true mean
gr <- cov.est[[i]]$mu.obj$yao$workGrid
lines(gr, mu(gr), col = 2, lwd = 2)
with(cov.est[[i]]$mu.obj$yao, lines(workGrid, mu, lwd = 2, col = 4))   # estimated mean curve
legend("topleft", c("True mean","Estimated mean"), col = c(2,4), lty = c(1,1))
```

```{r fig2, fig.cap = "Figure 2. True covariance surface and estimated covariance surfaces."}
### Covariance surface
work.grid <- cov.est[[i]]$work.grid
cov.list <- cov.est[[i]]$cov
par(mfrow = c(1, 3),
    mar = c(0, 2, 7, 2))
persp3D(work.grid, work.grid, cov.list$true, 
        theta = -70, phi = 30, expand = 1,
        main = "True", xlab = "s", ylab = "t", zlab = "C(s,t)")
persp3D(work.grid, work.grid, cov.list$yao, 
        theta = -70, phi = 30, expand = 1,
        main = "Yao et al. (2005)", xlab = "s", ylab = "t", zlab = "C(s,t)")
persp3D(work.grid, work.grid, cov.list$lin, 
        theta = -70, phi = 30, expand = 1,
        main = "Lin & Wang (2020)", xlab = "s", ylab = "t", zlab = "C(s,t)")
```



```{r tab1}
#############################
### Calculate RMISE
#############################
cname <- c("Yao (2005)","Lin (2020)")
rname <- c("RMISE","RMSE")
### variance
ise.var <- sapply(cov.est, function(x) {
  c(get_ise(diag(x$cov$true), diag(x$cov$yao), x$work.grid),
    get_ise(diag(x$cov$true), diag(x$cov$lin), x$work.grid))
})
mse.var <- sapply(cov.est, function(x) {
  c(mean((diag(x$cov$true) - diag(x$cov$yao))^2),
    mean((diag(x$cov$true) - diag(x$cov$lin))^2))
})

### covariance
ise.cov <- sapply(cov.est, function(x) {
  c(get_ise(x$cov$true, x$cov$yao, x$work.grid),
    get_ise(x$cov$true, x$cov$lin, x$work.grid))
})
mse.cov <- sapply(cov.est, function(x) {
  c(mean((x$cov$true - x$cov$yao)^2),
    mean((x$cov$true - x$cov$lin)^2))
})

res.mat <- cbind(rbind(sqrt(rowMeans(ise.var)), 
                       sqrt(rowMeans(mse.var))),
                 rbind(sqrt(rowMeans(ise.cov)),
                       sqrt(rowMeans(mse.cov))))
row.names(res.mat) <- rname
colnames(res.mat) <- c(cname, cname)

knitr::kable(res.mat, digits = 3, align = "c", caption = "Table 1. RMISE and RMSE for variance and covariance estimation.") %>% 
  kable_styling("striped", full_width = FALSE, font_size = 14) %>% 
  add_header_above(c(" " = 1, "Variance" = 2, "Covariance" = 2))
```


## Add outliers
- Outlier는 wiener process로부터 생성 후 mean function을 더해줌 (기존은 gaussian process)
- 3가지 mean function에 따라 결과 확인 (기존은 $\mu(t) = 2t^2\cos(2\pi t)$)
  - $\mu_1(t) = -2t^2 \cos(2\pi t)$ ($y$축 기준으로 대칭인 형태)
  - $\mu_2(t) = -3\sin(4\pi t)$ (fluctuation이 다른 형태)
  - $\mu_3(t) = 2(t-0.2)^2\cos(2\pi(t-0.2))$ ($x$축으로 평행이동한 형태)

### Outlier 1

```{r fig3, fig.cap = "Figure 3. Design plot (Left) and smaple trajectories with mean curve (Right)."}
load("RData/20210125_1.RData")

#############################
### Visualization with outliers
#############################
# i <- 100
x <- data.list.outlier[[i]]$x

### Design plot and sample trajectories
par(mfrow = c(1, 2))
CreateDesignPlot(x$t, main = "")
plot(x$t[[1]], x$y[[1]], type = "l", col = "grey",
     xlim = range(unlist(x$t)), ylim = range(unlist(x$y)), xlab = "Time", ylab = "")
for (j in 2:n) {
  if (j > n-n.outlier) {
    lines(x$t[[j]], x$y[[j]], col = 1)
  } else {
    lines(x$t[[j]], x$y[[j]], col = "grey")
  }
}
mu <- function(s) { 2*(s^2)*cos(2*pi*s) }   # true mean
mu.outlier <- function(s) { -2*(s^2)*cos(2*pi*s) }   # outlier 1
gr <- cov.est.outlier[[i]]$mu.obj$yao$workGrid
lines(gr, mu(gr), col = 2, lwd = 2)
lines(gr, mu.outlier(gr), col = 3, lwd = 2)
with(cov.est.outlier[[i]]$mu.obj$yao, lines(workGrid, mu, lwd = 2, col = 4))   # estimated mean curve
legend("topleft", c("True mean of non-outliers","True mean of outliers","Estimated mean"), col = c(2,3,4), lty = c(1,1,1))
```

```{r fig4, fig.cap = "Figure 4. True covariance surface and estimated covariance surfaces."}
### Covariance surface
work.grid <- cov.est.outlier[[i]]$work.grid
cov.list <- cov.est.outlier[[i]]$cov
par(mfrow = c(1, 3),
    mar = c(0, 2, 7, 2))
persp3D(work.grid, work.grid, cov.list$true, 
        theta = -70, phi = 30, expand = 1,
        main = "True", xlab = "s", ylab = "t", zlab = "C(s,t)")
persp3D(work.grid, work.grid, cov.list$yao, 
        theta = -70, phi = 30, expand = 1,
        main = "Yao et al. (2005)", xlab = "s", ylab = "t", zlab = "C(s,t)")
persp3D(work.grid, work.grid, cov.list$lin, 
        theta = -70, phi = 30, expand = 1,
        main = "Lin & Wang (2020)", xlab = "s", ylab = "t", zlab = "C(s,t)")
```


```{r tab2}
#############################
### Calculate RMISE with outliers
#############################

### variance
ise.var <- sapply(cov.est.outlier, function(x) {
  c(get_ise(diag(x$cov$true), diag(x$cov$yao), x$work.grid),
    get_ise(diag(x$cov$true), diag(x$cov$lin), x$work.grid))
})
mse.var <- sapply(cov.est.outlier, function(x) {
  c(mean((diag(x$cov$true) - diag(x$cov$yao))^2),
    mean((diag(x$cov$true) - diag(x$cov$lin))^2))
})

### covariance
ise.cov <- sapply(cov.est.outlier, function(x) {
  c(get_ise(x$cov$true, x$cov$yao, x$work.grid),
    get_ise(x$cov$true, x$cov$lin, x$work.grid))
})
mse.cov <- sapply(cov.est.outlier, function(x) {
  c(mean((x$cov$true - x$cov$yao)^2),
    mean((x$cov$true - x$cov$lin)^2))
})


res.mat <- cbind(rbind(sqrt(rowMeans(ise.var)), 
                       sqrt(rowMeans(mse.var))),
                 rbind(sqrt(rowMeans(ise.cov)),
                       sqrt(rowMeans(mse.cov))))
row.names(res.mat) <- rname
colnames(res.mat) <- c(cname, cname)

knitr::kable(res.mat, digits = 3, align = "c", caption = "Table 2. RMISE and RMSE for variance and covariance estimation.") %>% 
  kable_styling("striped", full_width = FALSE, font_size = 14) %>% 
  add_header_above(c(" " = 1, "Variance" = 2, "Covariance" = 2))
```



### Outlier 2

```{r fig5, fig.cap = "Figure 5. Design plot (Left) and smaple trajectories with mean curve (Right)."}
load("RData/20210125_2.RData")

#############################
### Visualization with outliers
#############################
i <- 1
x <- data.list.outlier[[i]]$x

### Design plot and sample trajectories
par(mfrow = c(1, 2))
CreateDesignPlot(x$t, main = "")
plot(x$t[[1]], x$y[[1]], type = "l", col = "grey",
     xlim = range(unlist(x$t)), ylim = range(unlist(x$y)), xlab = "Time", ylab = "")
for (j in 2:n) {
  if (j > n-n.outlier) {
    lines(x$t[[j]], x$y[[j]], col = 1)
  } else {
    lines(x$t[[j]], x$y[[j]], col = "grey")
  }
}
mu <- function(s) { 2*(s^2)*cos(2*pi*s) }   # true mean
mu.outlier <- function(s) { -3*sin(4*pi*s) }   # oulier 2
gr <- cov.est.outlier[[i]]$mu.obj$yao$workGrid
lines(gr, mu(gr), col = 2, lwd = 2)
lines(gr, mu.outlier(gr), col = 3, lwd = 2)
with(cov.est.outlier[[i]]$mu.obj$yao, lines(workGrid, mu, lwd = 2, col = 4))   # estimated mean curve
legend("topleft", c("True mean of non-outliers","True mean of outliers","Estimated mean"), col = c(2,3,4), lty = c(1,1,1))
```

```{r fig6, fig.cap = "Figure 6. True covariance surface and estimated covariance surfaces."}
### Covariance surface
work.grid <- cov.est.outlier[[i]]$work.grid
cov.list <- cov.est.outlier[[i]]$cov
par(mfrow = c(1, 3),
    mar = c(0, 2, 7, 2))
persp3D(work.grid, work.grid, cov.list$true, 
        theta = -70, phi = 30, expand = 1,
        main = "True", xlab = "s", ylab = "t", zlab = "C(s,t)")
persp3D(work.grid, work.grid, cov.list$yao, 
        theta = -70, phi = 30, expand = 1,
        main = "Yao et al. (2005)", xlab = "s", ylab = "t", zlab = "C(s,t)")
persp3D(work.grid, work.grid, cov.list$lin, 
        theta = -70, phi = 30, expand = 1,
        main = "Lin & Wang (2020)", xlab = "s", ylab = "t", zlab = "C(s,t)")
```


```{r tab3}
#############################
### Calculate RMISE with outliers
#############################

### variance
ise.var <- sapply(cov.est.outlier, function(x) {
  c(get_ise(diag(x$cov$true), diag(x$cov$yao), x$work.grid),
    get_ise(diag(x$cov$true), diag(x$cov$lin), x$work.grid))
})
mse.var <- sapply(cov.est.outlier, function(x) {
  c(mean((diag(x$cov$true) - diag(x$cov$yao))^2),
    mean((diag(x$cov$true) - diag(x$cov$lin))^2))
})

### covariance
ise.cov <- sapply(cov.est.outlier, function(x) {
  c(get_ise(x$cov$true, x$cov$yao, x$work.grid),
    get_ise(x$cov$true, x$cov$lin, x$work.grid))
})
mse.cov <- sapply(cov.est.outlier, function(x) {
  c(mean((x$cov$true - x$cov$yao)^2),
    mean((x$cov$true - x$cov$lin)^2))
})


res.mat <- cbind(rbind(sqrt(rowMeans(ise.var)), 
                       sqrt(rowMeans(mse.var))),
                 rbind(sqrt(rowMeans(ise.cov)),
                       sqrt(rowMeans(mse.cov))))
row.names(res.mat) <- rname
colnames(res.mat) <- c(cname, cname)

knitr::kable(res.mat, digits = 3, align = "c", caption = "Table 3. RMISE and RMSE for variance and covariance estimation.") %>% 
  kable_styling("striped", full_width = FALSE, font_size = 14) %>% 
  add_header_above(c(" " = 1, "Variance" = 2, "Covariance" = 2))
```


### Outlier 3

```{r fig7, fig.cap = "Figure 7. Design plot (Left) and smaple trajectories with mean curve (Right)."}
load("RData/20210125_3.RData")

#############################
### Visualization with outliers
#############################
i <- 1
x <- data.list.outlier[[i]]$x

### Design plot and sample trajectories
par(mfrow = c(1, 2))
CreateDesignPlot(x$t, main = "")
plot(x$t[[1]], x$y[[1]], type = "l", col = "grey",
     xlim = range(unlist(x$t)), ylim = range(unlist(x$y)), xlab = "Time", ylab = "")
for (j in 2:n) {
  if (j > n-n.outlier) {
    lines(x$t[[j]], x$y[[j]], col = 1)
  } else {
    lines(x$t[[j]], x$y[[j]], col = "grey")
  }
}
mu <- function(s) { 2*(s^2)*cos(2*pi*s) }   # true mean
mu.outlier <- function(s) { 2*((s-0.2)^2)*cos(2*pi*(s-0.2)) }   # outlier 3
gr <- cov.est.outlier[[i]]$mu.obj$yao$workGrid
lines(gr, mu(gr), col = 2, lwd = 2)
lines(gr, mu.outlier(gr), col = 3, lwd = 2)
with(cov.est.outlier[[i]]$mu.obj$yao, lines(workGrid, mu, lwd = 2, col = 4))   # estimated mean curve
legend("topleft", c("True mean of non-outliers","True mean of outliers","Estimated mean"), col = c(2,3,4), lty = c(1,1,1))
```

```{r fig8, fig.cap = "Figure 8. True covariance surface and estimated covariance surfaces."}
### Covariance surface
work.grid <- cov.est.outlier[[i]]$work.grid
cov.list <- cov.est.outlier[[i]]$cov
par(mfrow = c(1, 3),
    mar = c(0, 2, 7, 2))
persp3D(work.grid, work.grid, cov.list$true, 
        theta = -70, phi = 30, expand = 1,
        main = "True", xlab = "s", ylab = "t", zlab = "C(s,t)")
persp3D(work.grid, work.grid, cov.list$yao, 
        theta = -70, phi = 30, expand = 1,
        main = "Yao et al. (2005)", xlab = "s", ylab = "t", zlab = "C(s,t)")
persp3D(work.grid, work.grid, cov.list$lin, 
        theta = -70, phi = 30, expand = 1,
        main = "Lin & Wang (2020)", xlab = "s", ylab = "t", zlab = "C(s,t)")
```


```{r tab4}
#############################
### Calculate RMISE with outliers
#############################

### variance
ise.var <- sapply(cov.est.outlier, function(x) {
  c(get_ise(diag(x$cov$true), diag(x$cov$yao), x$work.grid),
    get_ise(diag(x$cov$true), diag(x$cov$lin), x$work.grid))
})
mse.var <- sapply(cov.est.outlier, function(x) {
  c(mean((diag(x$cov$true) - diag(x$cov$yao))^2),
    mean((diag(x$cov$true) - diag(x$cov$lin))^2))
})

### covariance
ise.cov <- sapply(cov.est.outlier, function(x) {
  c(get_ise(x$cov$true, x$cov$yao, x$work.grid),
    get_ise(x$cov$true, x$cov$lin, x$work.grid))
})
mse.cov <- sapply(cov.est.outlier, function(x) {
  c(mean((x$cov$true - x$cov$yao)^2),
    mean((x$cov$true - x$cov$lin)^2))
})


res.mat <- cbind(rbind(sqrt(rowMeans(ise.var)), 
                       sqrt(rowMeans(mse.var))),
                 rbind(sqrt(rowMeans(ise.cov)),
                       sqrt(rowMeans(mse.cov))))
row.names(res.mat) <- rname
colnames(res.mat) <- c(cname, cname)

knitr::kable(res.mat, digits = 3, align = "c", caption = "Table 4. RMISE and RMSE for variance and covariance estimation.") %>% 
  kable_styling("striped", full_width = FALSE, font_size = 14) %>% 
  add_header_above(c(" " = 1, "Variance" = 2, "Covariance" = 2))
```

<br>

# Conclusion
- Variance, covariance estimation에서 모두 `Lin & Wang (2020)`이 `Yao et al. (2005)`보다 낮은 RMISE, RMSE를 보여주었음.
- `Yao et al. (2005)`의 경우, outlier가 추가되면서 off-diagonal 부분에서 RMISE의 증가폭이 매우 컸음.
- **Outlier 2**의 경우, 두 방법에서 variance의 RMISE, RMSE가 줄어드는 경향이 있었지만, covariance에서 두 값이 큰 폭으로 증가함.
